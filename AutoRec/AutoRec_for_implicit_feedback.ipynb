{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoRec-for-implicit-feedback.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NR3Q7bKCQaa_LqFUlzXBLu6wIPXHGkAn",
      "authorship_tag": "ABX9TyMuMyJZEVeJ9IVfHX0fvKVf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/AutoRec/AutoRec_for_implicit_feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzJgxsrewLBy",
        "outputId": "b71030b5-53fb-460e-d115-795d1e0332ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-box\n",
            "  Downloading python_box-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug-br-pPu9vZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbRKDSg4u9vc"
      },
      "source": [
        "# 1. 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlm1UrKvoC_O"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n",
        "\n",
        "    'model_path' : \"/content/drive/MyDrive/RecsysTutorial/model\", # 모델 저장 경로\n",
        "    'model_name' : 'AutoRec.pt',\n",
        "\n",
        "    'num_factor': 64,\n",
        "\n",
        "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
        "    'seed' : 22,\n",
        "\n",
        "    'lr' : 0.005,\n",
        "    'batch_size' : 128,\n",
        "    'num_epochs' : 200,\n",
        "    'num_workers' : 2,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqVUlLd9u9ve"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(config.model_path):\n",
        "    os.mkdir(config.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDxy0fJu9vf"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W64BYWl0u9vg"
      },
      "outputs": [],
      "source": [
        "class MakeMatrixDataSet():\n",
        "    \"\"\"\n",
        "    MatrixDataSet 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n",
        "        \n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder[x])\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
        "            users[user].append(item)\n",
        "        \n",
        "        for user in users:\n",
        "            np.random.seed(self.config.seed)\n",
        "\n",
        "            user_total = users[user]\n",
        "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
        "            train = list(set(user_total) - set(valid))\n",
        "\n",
        "            user_train[user] = train\n",
        "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
        "\n",
        "        return user_train, user_valid\n",
        "    \n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid\n",
        "\n",
        "    def make_matrix(self, user_list, train = True):\n",
        "        \"\"\"\n",
        "        user_item_dict를 바탕으로 행렬 생성\n",
        "        \"\"\"\n",
        "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
        "        for idx, user in enumerate(user_list):\n",
        "            if train:\n",
        "                mat[idx, self.user_train[user.item()]] = 1\n",
        "            else:\n",
        "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
        "        return mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IldCGmY8u9vh"
      },
      "outputs": [],
      "source": [
        "class AEDataSet(Dataset):\n",
        "    def __init__(self, num_user):\n",
        "        self.num_user = num_user\n",
        "        self.users = [i for i in range(num_user)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        user = self.users[idx]\n",
        "        return torch.LongTensor([user])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysia457Su9vi"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EgjAJ-ju9vj"
      },
      "outputs": [],
      "source": [
        "class AutoRec(nn.Module):\n",
        "    def __init__(self, num, num_factor):\n",
        "        super(AutoRec, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(num, num_factor),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_factor, num_factor // 2),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(num_factor // 2, num_factor),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(num_factor, num),\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, mat):\n",
        "        latent = self.encoder(mat)\n",
        "        recont_mat = self.decoder(latent)\n",
        "\n",
        "        return recont_mat\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.encoder:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                size = layer.weight.size()\n",
        "                fan_out = size[0]\n",
        "                fan_in = size[1]\n",
        "                std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "                layer.weight.data.normal_(0.0, std)\n",
        "                layer.bias.data.normal_(0.0, 0.001)\n",
        "        \n",
        "        for layer in self.decoder:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                size = layer.weight.size()\n",
        "                fan_out = size[0]\n",
        "                fan_in = size[1]\n",
        "                std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "                layer.weight.data.normal_(0.0, std)\n",
        "                layer.bias.data.normal_(0.0, 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw2LhYkau9vj"
      },
      "source": [
        "# 4. 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmooa3n1u9vj"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, data_loader, make_matrix_data_set):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for users in data_loader:\n",
        "        mat = make_matrix_data_set.make_matrix(users)\n",
        "        mat = mat.to(device)\n",
        "        recon_mat = model(mat)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(recon_mat, mat)\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def get_ndcg(pred_list, true_list):\n",
        "    ndcg = 0\n",
        "    for rank, pred in enumerate(pred_list):\n",
        "        if pred in true_list:\n",
        "            ndcg += 1 / np.log2(rank + 2)\n",
        "    return ndcg\n",
        "\n",
        "# hit == recall == precision\n",
        "def get_hit(pred_list, true_list):\n",
        "    hit_list = set(true_list) & set(pred_list)\n",
        "    hit = len(hit_list) / len(true_list)\n",
        "    return hit\n",
        "\n",
        "def evaluate(model, data_loader, user_train, user_valid, make_matrix_data_set):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for users in data_loader:\n",
        "            mat = make_matrix_data_set.make_matrix(users)\n",
        "            mat = mat.to(device)\n",
        "\n",
        "            recon_mat = model(mat)\n",
        "            recon_mat = recon_mat.softmax(dim = 1)\n",
        "            recon_mat[mat == 1] = -1.\n",
        "            rec_list = recon_mat.argsort(dim = 1)\n",
        "\n",
        "            for user, rec in zip(users, rec_list):\n",
        "                uv = user_valid[user.item()]\n",
        "                up = rec[-10:].cpu().numpy().tolist()\n",
        "                NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
        "                HIT += get_hit(pred_list = up, true_list = uv)\n",
        "\n",
        "    NDCG /= len(data_loader.dataset)\n",
        "    HIT /= len(data_loader.dataset)\n",
        "\n",
        "    return NDCG, HIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwSexh43u9vk"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zXVEf6fu9vk"
      },
      "outputs": [],
      "source": [
        "make_matrix_data_set = MakeMatrixDataSet(config = config)\n",
        "user_train, user_valid = make_matrix_data_set.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hGPyH54u9vk"
      },
      "outputs": [],
      "source": [
        "ae_dataset = AEDataSet(\n",
        "    num_user = make_matrix_data_set.num_user,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyL3vriiu9vl"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    ae_dataset,\n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpiWnBV8u9vm"
      },
      "outputs": [],
      "source": [
        "model = AutoRec(\n",
        "    num = make_matrix_data_set.num_item, \n",
        "    num_factor = config.num_factor).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_MtN5IOu9vm",
        "outputId": "d1726d89-f7ba-4d13-b45c-6f3a95aa3330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.01890| NDCG@10: 0.25544| HIT@10: 0.06200: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
            "Epoch:   2| Train loss: 0.01554| NDCG@10: 0.25896| HIT@10: 0.06230: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
            "Epoch:   3| Train loss: 0.01389| NDCG@10: 0.24409| HIT@10: 0.05961: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Epoch:   4| Train loss: 0.01454| NDCG@10: 0.22810| HIT@10: 0.05618: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "Epoch:   5| Train loss: 0.01405| NDCG@10: 0.24902| HIT@10: 0.06036: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
            "Epoch:   6| Train loss: 0.01336| NDCG@10: 0.24866| HIT@10: 0.06200: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch:   7| Train loss: 0.01218| NDCG@10: 0.25641| HIT@10: 0.06304: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Epoch:   8| Train loss: 0.01240| NDCG@10: 0.25384| HIT@10: 0.06274: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Epoch:   9| Train loss: 0.01292| NDCG@10: 0.27865| HIT@10: 0.06513: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch:  10| Train loss: 0.01347| NDCG@10: 0.26455| HIT@10: 0.06393: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n",
            "Epoch:  11| Train loss: 0.01230| NDCG@10: 0.25178| HIT@10: 0.06304: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n",
            "Epoch:  12| Train loss: 0.01287| NDCG@10: 0.25686| HIT@10: 0.06364: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
            "Epoch:  13| Train loss: 0.01194| NDCG@10: 0.26673| HIT@10: 0.06468: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]\n",
            "Epoch:  14| Train loss: 0.01168| NDCG@10: 0.26963| HIT@10: 0.06453: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch:  15| Train loss: 0.01251| NDCG@10: 0.27565| HIT@10: 0.06498: 100%|██████████| 1/1 [00:00<00:00,  2.49it/s]\n",
            "Epoch:  16| Train loss: 0.01249| NDCG@10: 0.26622| HIT@10: 0.06438: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Epoch:  17| Train loss: 0.01270| NDCG@10: 0.25314| HIT@10: 0.06349: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch:  18| Train loss: 0.01211| NDCG@10: 0.27719| HIT@10: 0.06542: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  19| Train loss: 0.01146| NDCG@10: 0.27120| HIT@10: 0.06602: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch:  20| Train loss: 0.01268| NDCG@10: 0.27662| HIT@10: 0.06766: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
            "Epoch:  21| Train loss: 0.01130| NDCG@10: 0.28428| HIT@10: 0.06885: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch:  22| Train loss: 0.01113| NDCG@10: 0.29177| HIT@10: 0.07213: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
            "Epoch:  23| Train loss: 0.01147| NDCG@10: 0.29455| HIT@10: 0.07198: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
            "Epoch:  24| Train loss: 0.01145| NDCG@10: 0.30326| HIT@10: 0.07332: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch:  25| Train loss: 0.01119| NDCG@10: 0.31120| HIT@10: 0.07586: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Epoch:  26| Train loss: 0.01158| NDCG@10: 0.33128| HIT@10: 0.08048: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch:  27| Train loss: 0.01126| NDCG@10: 0.34366| HIT@10: 0.08301: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
            "Epoch:  28| Train loss: 0.01096| NDCG@10: 0.35245| HIT@10: 0.08420: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
            "Epoch:  29| Train loss: 0.01096| NDCG@10: 0.36296| HIT@10: 0.08718: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  30| Train loss: 0.01113| NDCG@10: 0.37727| HIT@10: 0.08897: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch:  31| Train loss: 0.01099| NDCG@10: 0.39762| HIT@10: 0.09285: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch:  32| Train loss: 0.01116| NDCG@10: 0.39033| HIT@10: 0.09314: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch:  33| Train loss: 0.01108| NDCG@10: 0.40557| HIT@10: 0.09762: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Epoch:  34| Train loss: 0.01064| NDCG@10: 0.41061| HIT@10: 0.09866: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Epoch:  35| Train loss: 0.01073| NDCG@10: 0.41439| HIT@10: 0.09925: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch:  36| Train loss: 0.01139| NDCG@10: 0.44659| HIT@10: 0.10432: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
            "Epoch:  37| Train loss: 0.01091| NDCG@10: 0.43544| HIT@10: 0.10343: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch:  38| Train loss: 0.01041| NDCG@10: 0.44638| HIT@10: 0.10507: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Epoch:  39| Train loss: 0.01087| NDCG@10: 0.44343| HIT@10: 0.10447: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch:  40| Train loss: 0.01086| NDCG@10: 0.44677| HIT@10: 0.10611: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Epoch:  41| Train loss: 0.01107| NDCG@10: 0.45935| HIT@10: 0.10775: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
            "Epoch:  42| Train loss: 0.01116| NDCG@10: 0.47336| HIT@10: 0.10984: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  43| Train loss: 0.01078| NDCG@10: 0.45955| HIT@10: 0.10864: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch:  44| Train loss: 0.01069| NDCG@10: 0.47190| HIT@10: 0.10924: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch:  45| Train loss: 0.01085| NDCG@10: 0.47579| HIT@10: 0.11058: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
            "Epoch:  46| Train loss: 0.01115| NDCG@10: 0.46760| HIT@10: 0.11013: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch:  47| Train loss: 0.01058| NDCG@10: 0.46638| HIT@10: 0.11073: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
            "Epoch:  48| Train loss: 0.01053| NDCG@10: 0.46099| HIT@10: 0.10924: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch:  49| Train loss: 0.01038| NDCG@10: 0.47158| HIT@10: 0.11118: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n",
            "Epoch:  50| Train loss: 0.01076| NDCG@10: 0.45489| HIT@10: 0.11043: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch:  51| Train loss: 0.01062| NDCG@10: 0.47377| HIT@10: 0.11207: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
            "Epoch:  52| Train loss: 0.01024| NDCG@10: 0.47343| HIT@10: 0.11252: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Epoch:  53| Train loss: 0.01100| NDCG@10: 0.46516| HIT@10: 0.11192: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch:  54| Train loss: 0.01031| NDCG@10: 0.47160| HIT@10: 0.11356: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Epoch:  55| Train loss: 0.01038| NDCG@10: 0.48281| HIT@10: 0.11431: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
            "Epoch:  56| Train loss: 0.01028| NDCG@10: 0.48609| HIT@10: 0.11461: 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
            "Epoch:  57| Train loss: 0.01064| NDCG@10: 0.48292| HIT@10: 0.11431: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
            "Epoch:  58| Train loss: 0.00997| NDCG@10: 0.47243| HIT@10: 0.11297: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  59| Train loss: 0.01023| NDCG@10: 0.48006| HIT@10: 0.11446: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch:  60| Train loss: 0.01009| NDCG@10: 0.46908| HIT@10: 0.11311: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch:  61| Train loss: 0.01029| NDCG@10: 0.48707| HIT@10: 0.11610: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Epoch:  62| Train loss: 0.00999| NDCG@10: 0.48947| HIT@10: 0.11759: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n",
            "Epoch:  63| Train loss: 0.00961| NDCG@10: 0.48133| HIT@10: 0.11580: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch:  64| Train loss: 0.00986| NDCG@10: 0.49381| HIT@10: 0.11848: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Epoch:  65| Train loss: 0.00976| NDCG@10: 0.47528| HIT@10: 0.11550: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  66| Train loss: 0.01026| NDCG@10: 0.49517| HIT@10: 0.11818: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch:  67| Train loss: 0.00964| NDCG@10: 0.49040| HIT@10: 0.11803: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Epoch:  68| Train loss: 0.00968| NDCG@10: 0.50329| HIT@10: 0.12191: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch:  69| Train loss: 0.00971| NDCG@10: 0.48999| HIT@10: 0.11967: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  70| Train loss: 0.01014| NDCG@10: 0.49742| HIT@10: 0.11982: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch:  71| Train loss: 0.00939| NDCG@10: 0.50834| HIT@10: 0.12146: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch:  72| Train loss: 0.00987| NDCG@10: 0.49043| HIT@10: 0.11997: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Epoch:  73| Train loss: 0.00937| NDCG@10: 0.48962| HIT@10: 0.11997: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch:  74| Train loss: 0.00987| NDCG@10: 0.51116| HIT@10: 0.12146: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Epoch:  75| Train loss: 0.00936| NDCG@10: 0.51047| HIT@10: 0.12385: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Epoch:  76| Train loss: 0.00904| NDCG@10: 0.50952| HIT@10: 0.12370: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch:  77| Train loss: 0.00946| NDCG@10: 0.52950| HIT@10: 0.12563: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Epoch:  78| Train loss: 0.00964| NDCG@10: 0.51960| HIT@10: 0.12280: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch:  79| Train loss: 0.00948| NDCG@10: 0.51473| HIT@10: 0.12399: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch:  80| Train loss: 0.00916| NDCG@10: 0.50853| HIT@10: 0.12414: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
            "Epoch:  81| Train loss: 0.00952| NDCG@10: 0.53394| HIT@10: 0.12608: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
            "Epoch:  82| Train loss: 0.00904| NDCG@10: 0.52639| HIT@10: 0.12385: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch:  83| Train loss: 0.00939| NDCG@10: 0.52190| HIT@10: 0.12489: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch:  84| Train loss: 0.00887| NDCG@10: 0.52711| HIT@10: 0.12504: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch:  85| Train loss: 0.00937| NDCG@10: 0.51890| HIT@10: 0.12623: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch:  86| Train loss: 0.00915| NDCG@10: 0.52333| HIT@10: 0.12563: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch:  87| Train loss: 0.00908| NDCG@10: 0.55365| HIT@10: 0.13010: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Epoch:  88| Train loss: 0.00846| NDCG@10: 0.51993| HIT@10: 0.12638: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch:  89| Train loss: 0.00896| NDCG@10: 0.53472| HIT@10: 0.12876: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch:  90| Train loss: 0.00893| NDCG@10: 0.53456| HIT@10: 0.12832: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch:  91| Train loss: 0.00873| NDCG@10: 0.56080| HIT@10: 0.13040: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch:  92| Train loss: 0.00865| NDCG@10: 0.55253| HIT@10: 0.12996: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch:  93| Train loss: 0.00926| NDCG@10: 0.56148| HIT@10: 0.13174: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch:  94| Train loss: 0.00920| NDCG@10: 0.53081| HIT@10: 0.12623: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch:  95| Train loss: 0.00893| NDCG@10: 0.56279| HIT@10: 0.13249: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch:  96| Train loss: 0.00883| NDCG@10: 0.53974| HIT@10: 0.13070: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch:  97| Train loss: 0.00877| NDCG@10: 0.54857| HIT@10: 0.13115: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Epoch:  98| Train loss: 0.00872| NDCG@10: 0.52765| HIT@10: 0.12846: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch:  99| Train loss: 0.00888| NDCG@10: 0.54643| HIT@10: 0.13055: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch: 100| Train loss: 0.00837| NDCG@10: 0.54264| HIT@10: 0.13025: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 101| Train loss: 0.00873| NDCG@10: 0.52713| HIT@10: 0.12787: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Epoch: 102| Train loss: 0.00836| NDCG@10: 0.55941| HIT@10: 0.13189: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 103| Train loss: 0.00875| NDCG@10: 0.55158| HIT@10: 0.13130: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch: 104| Train loss: 0.00859| NDCG@10: 0.53619| HIT@10: 0.12951: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 105| Train loss: 0.00836| NDCG@10: 0.53511| HIT@10: 0.12906: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 106| Train loss: 0.00842| NDCG@10: 0.54026| HIT@10: 0.12951: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 107| Train loss: 0.00853| NDCG@10: 0.55685| HIT@10: 0.13219: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch: 108| Train loss: 0.00864| NDCG@10: 0.54545| HIT@10: 0.12981: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch: 109| Train loss: 0.00863| NDCG@10: 0.54357| HIT@10: 0.13204: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 110| Train loss: 0.00857| NDCG@10: 0.55833| HIT@10: 0.13234: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Epoch: 111| Train loss: 0.00845| NDCG@10: 0.54330| HIT@10: 0.13115: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 112| Train loss: 0.00847| NDCG@10: 0.53834| HIT@10: 0.13115: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch: 113| Train loss: 0.00837| NDCG@10: 0.55564| HIT@10: 0.13219: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 114| Train loss: 0.00841| NDCG@10: 0.57019| HIT@10: 0.13651: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Epoch: 115| Train loss: 0.00818| NDCG@10: 0.56441| HIT@10: 0.13398: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
            "Epoch: 116| Train loss: 0.00845| NDCG@10: 0.55254| HIT@10: 0.13428: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Epoch: 117| Train loss: 0.00828| NDCG@10: 0.53852| HIT@10: 0.13085: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch: 118| Train loss: 0.00822| NDCG@10: 0.58482| HIT@10: 0.13651: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Epoch: 119| Train loss: 0.00798| NDCG@10: 0.56763| HIT@10: 0.13562: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 120| Train loss: 0.00801| NDCG@10: 0.56214| HIT@10: 0.13532: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 121| Train loss: 0.00827| NDCG@10: 0.56467| HIT@10: 0.13636: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch: 122| Train loss: 0.00807| NDCG@10: 0.56022| HIT@10: 0.13607: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 123| Train loss: 0.00803| NDCG@10: 0.56662| HIT@10: 0.13592: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 124| Train loss: 0.00826| NDCG@10: 0.56379| HIT@10: 0.13666: 100%|██████████| 1/1 [00:00<00:00,  2.16it/s]\n",
            "Epoch: 125| Train loss: 0.00798| NDCG@10: 0.58155| HIT@10: 0.13830: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Epoch: 126| Train loss: 0.00807| NDCG@10: 0.55903| HIT@10: 0.13547: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
            "Epoch: 127| Train loss: 0.00805| NDCG@10: 0.56655| HIT@10: 0.13636: 100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Epoch: 128| Train loss: 0.00813| NDCG@10: 0.55467| HIT@10: 0.13741: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 129| Train loss: 0.00799| NDCG@10: 0.56158| HIT@10: 0.13621: 100%|██████████| 1/1 [00:00<00:00,  2.44it/s]\n",
            "Epoch: 130| Train loss: 0.00806| NDCG@10: 0.55273| HIT@10: 0.13413: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 131| Train loss: 0.00801| NDCG@10: 0.56582| HIT@10: 0.13785: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Epoch: 132| Train loss: 0.00798| NDCG@10: 0.56200| HIT@10: 0.13726: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Epoch: 133| Train loss: 0.00782| NDCG@10: 0.56529| HIT@10: 0.13696: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 134| Train loss: 0.00845| NDCG@10: 0.56153| HIT@10: 0.13562: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 135| Train loss: 0.00838| NDCG@10: 0.55739| HIT@10: 0.13517: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
            "Epoch: 136| Train loss: 0.00811| NDCG@10: 0.56643| HIT@10: 0.13770: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch: 137| Train loss: 0.00814| NDCG@10: 0.56623| HIT@10: 0.13741: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Epoch: 138| Train loss: 0.00780| NDCG@10: 0.56537| HIT@10: 0.13726: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 139| Train loss: 0.00781| NDCG@10: 0.56673| HIT@10: 0.13815: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Epoch: 140| Train loss: 0.00789| NDCG@10: 0.56949| HIT@10: 0.13785: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch: 141| Train loss: 0.00784| NDCG@10: 0.54939| HIT@10: 0.13413: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch: 142| Train loss: 0.00809| NDCG@10: 0.58094| HIT@10: 0.14113: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
            "Epoch: 143| Train loss: 0.00787| NDCG@10: 0.57702| HIT@10: 0.13949: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 144| Train loss: 0.00779| NDCG@10: 0.57293| HIT@10: 0.13905: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 145| Train loss: 0.00765| NDCG@10: 0.59576| HIT@10: 0.14262: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Epoch: 146| Train loss: 0.00771| NDCG@10: 0.56923| HIT@10: 0.13890: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 147| Train loss: 0.00768| NDCG@10: 0.56743| HIT@10: 0.13890: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch: 148| Train loss: 0.00766| NDCG@10: 0.57342| HIT@10: 0.14054: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 149| Train loss: 0.00749| NDCG@10: 0.61118| HIT@10: 0.14441: 100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
            "Epoch: 150| Train loss: 0.00769| NDCG@10: 0.59945| HIT@10: 0.14322: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch: 151| Train loss: 0.00767| NDCG@10: 0.58713| HIT@10: 0.14173: 100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Epoch: 152| Train loss: 0.00771| NDCG@10: 0.57794| HIT@10: 0.13994: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 153| Train loss: 0.00765| NDCG@10: 0.57961| HIT@10: 0.14113: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 154| Train loss: 0.00753| NDCG@10: 0.59390| HIT@10: 0.14441: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch: 155| Train loss: 0.00771| NDCG@10: 0.58173| HIT@10: 0.14128: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 156| Train loss: 0.00767| NDCG@10: 0.60223| HIT@10: 0.14337: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 157| Train loss: 0.00775| NDCG@10: 0.58573| HIT@10: 0.14426: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Epoch: 158| Train loss: 0.00780| NDCG@10: 0.58448| HIT@10: 0.14098: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch: 159| Train loss: 0.00762| NDCG@10: 0.56726| HIT@10: 0.13845: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
            "Epoch: 160| Train loss: 0.00787| NDCG@10: 0.59621| HIT@10: 0.14232: 100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Epoch: 161| Train loss: 0.00755| NDCG@10: 0.57223| HIT@10: 0.13994: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch: 162| Train loss: 0.00739| NDCG@10: 0.60019| HIT@10: 0.14367: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 163| Train loss: 0.00751| NDCG@10: 0.60651| HIT@10: 0.14605: 100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Epoch: 164| Train loss: 0.00744| NDCG@10: 0.57630| HIT@10: 0.14188: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 165| Train loss: 0.00749| NDCG@10: 0.60926| HIT@10: 0.14605: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch: 166| Train loss: 0.00787| NDCG@10: 0.61301| HIT@10: 0.14709: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch: 167| Train loss: 0.00742| NDCG@10: 0.58199| HIT@10: 0.14143: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 168| Train loss: 0.00719| NDCG@10: 0.58887| HIT@10: 0.14307: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 169| Train loss: 0.00754| NDCG@10: 0.59897| HIT@10: 0.14545: 100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Epoch: 170| Train loss: 0.00750| NDCG@10: 0.60878| HIT@10: 0.14620: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 171| Train loss: 0.00735| NDCG@10: 0.62422| HIT@10: 0.14680: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch: 172| Train loss: 0.00732| NDCG@10: 0.61698| HIT@10: 0.14709: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Epoch: 173| Train loss: 0.00739| NDCG@10: 0.60801| HIT@10: 0.14560: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch: 174| Train loss: 0.00731| NDCG@10: 0.61167| HIT@10: 0.14650: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 175| Train loss: 0.00710| NDCG@10: 0.61290| HIT@10: 0.14680: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch: 176| Train loss: 0.00725| NDCG@10: 0.59024| HIT@10: 0.14396: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Epoch: 177| Train loss: 0.00716| NDCG@10: 0.60180| HIT@10: 0.14486: 100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Epoch: 178| Train loss: 0.00739| NDCG@10: 0.61415| HIT@10: 0.14620: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Epoch: 179| Train loss: 0.00708| NDCG@10: 0.61771| HIT@10: 0.14709: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 180| Train loss: 0.00731| NDCG@10: 0.59700| HIT@10: 0.14531: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Epoch: 181| Train loss: 0.00725| NDCG@10: 0.59356| HIT@10: 0.14516: 100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Epoch: 182| Train loss: 0.00731| NDCG@10: 0.61205| HIT@10: 0.14844: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Epoch: 183| Train loss: 0.00746| NDCG@10: 0.60392| HIT@10: 0.14516: 100%|██████████| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Epoch: 184| Train loss: 0.00751| NDCG@10: 0.59972| HIT@10: 0.14575: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 185| Train loss: 0.00719| NDCG@10: 0.59778| HIT@10: 0.14516: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Epoch: 186| Train loss: 0.00731| NDCG@10: 0.60948| HIT@10: 0.14694: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n",
            "Epoch: 187| Train loss: 0.00733| NDCG@10: 0.59470| HIT@10: 0.14471: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n",
            "Epoch: 188| Train loss: 0.00727| NDCG@10: 0.60995| HIT@10: 0.14709: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 189| Train loss: 0.00714| NDCG@10: 0.62316| HIT@10: 0.14858: 100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
            "Epoch: 190| Train loss: 0.00721| NDCG@10: 0.60820| HIT@10: 0.14620: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 191| Train loss: 0.00731| NDCG@10: 0.60156| HIT@10: 0.14531: 100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Epoch: 192| Train loss: 0.00709| NDCG@10: 0.60426| HIT@10: 0.14516: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 193| Train loss: 0.00709| NDCG@10: 0.61454| HIT@10: 0.14858: 100%|██████████| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Epoch: 194| Train loss: 0.00734| NDCG@10: 0.61198| HIT@10: 0.14799: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
            "Epoch: 195| Train loss: 0.00715| NDCG@10: 0.61510| HIT@10: 0.14903: 100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Epoch: 196| Train loss: 0.00686| NDCG@10: 0.59926| HIT@10: 0.14665: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Epoch: 197| Train loss: 0.00719| NDCG@10: 0.61454| HIT@10: 0.14829: 100%|██████████| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Epoch: 198| Train loss: 0.00718| NDCG@10: 0.60750| HIT@10: 0.14650: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch: 199| Train loss: 0.00721| NDCG@10: 0.60541| HIT@10: 0.14635: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
            "Epoch: 200| Train loss: 0.00714| NDCG@10: 0.61154| HIT@10: 0.14769: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n"
          ]
        }
      ],
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            data_loader = data_loader,\n",
        "            make_matrix_data_set = make_matrix_data_set\n",
        "            )\n",
        "        \n",
        "        ndcg, hit = evaluate(\n",
        "            model = model,\n",
        "            data_loader = data_loader,\n",
        "            user_train = user_train,\n",
        "            user_valid = user_valid,\n",
        "            make_matrix_data_set = make_matrix_data_set,\n",
        "            )\n",
        "\n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RO-l9Umn3t_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}