{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LightGCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mgSERscBiRwpZyPI2Ev4SOXE9wlQkLA2",
      "authorship_tag": "ABX9TyME9ScMq0rXfNhHN/tnjaSu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/LightGCN/LightGCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjOnv1NS52ww",
        "outputId": "70c8f2db-b228-4de8-bccb-3f43b3c6f347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-box\n",
            "  Downloading python_box-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtBCq174_N1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8uF4zk4_N2"
      },
      "source": [
        "# 1. 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S05ItDiE4-ia"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n",
        "\n",
        "    'model_path' : \"/content/drive/MyDrive/RecsysTutorial/model\", # 모델 저장 경로\n",
        "    'model_name' : 'LightGCN.pt',\n",
        "\n",
        "    'num_epochs' : 50,\n",
        "    \"reg\" : 1e-5,\n",
        "    'lr' : 0.0001,\n",
        "    \"emb_dim\" : 128,\n",
        "    \"n_layers\" : 3,\n",
        "    'batch_size' : 500,\n",
        "    \"node_dropout\" : 0.2,\n",
        "\n",
        "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
        "    'seed' : 22,\n",
        "    'n_batch' : 10,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1RD4CH4_N3"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(config.model_path):\n",
        "    os.mkdir(config.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlRF8eUo4_N3"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXbxctF04_N3"
      },
      "outputs": [],
      "source": [
        "class MakeGraphDataSet():\n",
        "    \"\"\"\n",
        "    GraphDataSet 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder[x])\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        \n",
        "        self.exist_users = [i for i in range(self.num_user)]\n",
        "        self.exist_items = [i for i in range(self.num_item)]\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
        "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
        "        self.n_train = len(self.R_train)\n",
        "        self.batch_size = self.config.batch_size\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
        "            users[user].append(item)\n",
        "        \n",
        "        for user in users:\n",
        "            np.random.seed(self.config.seed)\n",
        "\n",
        "            user_total = users[user]\n",
        "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
        "            train = list(set(user_total) - set(valid))\n",
        "\n",
        "            user_train[user] = train\n",
        "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
        "\n",
        "        return user_train, user_valid\n",
        "    \n",
        "    def generate_dok_matrix(self):\n",
        "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        user_list = self.exist_users\n",
        "        for user in user_list:\n",
        "            train_items = self.user_train[user]\n",
        "            valid_items = self.user_valid[user]\n",
        "            \n",
        "            for train_item in train_items:\n",
        "                R_train[user, train_item] = 1.0\n",
        "                R_total[user, train_item] = 1.0\n",
        "            \n",
        "            for valid_item in valid_items:\n",
        "                R_valid[user, valid_item] = 1.0\n",
        "                R_total[user, valid_item] = 1.0\n",
        "        \n",
        "        return R_train, R_valid, R_total\n",
        "\n",
        "    def generate_ngcf_adj_matrix(self):\n",
        "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
        "        adj_mat = adj_mat.tolil() # to_list\n",
        "        R = self.R_train.tolil()\n",
        "\n",
        "        adj_mat[:self.num_user, self.num_user:] = R\n",
        "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
        "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
        "\n",
        "        def normalized_adj_single(adj):\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            d_inv = np.power(rowsum, -.5).flatten()  \n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat_inv = sp.diags(d_inv)\n",
        "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
        "\n",
        "            return norm_adj.tocoo()\n",
        "\n",
        "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
        "        return ngcf_adj_matrix.tocsr()\n",
        "\n",
        "    def sampling(self):\n",
        "        users = random.sample(self.exist_users, self.config.batch_size)\n",
        "\n",
        "        def sample_pos_items_for_u(u, num):\n",
        "            pos_items = self.user_train[u]\n",
        "            pos_batch = random.sample(pos_items, num)\n",
        "            return pos_batch\n",
        "        \n",
        "        def sample_neg_items_for_u(u, num):\n",
        "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
        "            neg_batch = random.sample(neg_items, num)\n",
        "            return neg_batch\n",
        "        \n",
        "        pos_items, neg_items = [], []\n",
        "        for user in users:\n",
        "            pos_items += sample_pos_items_for_u(user, 1)\n",
        "            neg_items += sample_neg_items_for_u(user, 1)\n",
        "        \n",
        "        return users, pos_items, neg_items\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid\n",
        "\n",
        "    def get_R_data(self):\n",
        "        return self.R_train, self.R_valid, self.R_total\n",
        "\n",
        "    def get_ngcf_adj_matrix_data(self):\n",
        "        return self.ngcf_adj_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvj641L04-id"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhaTU8Gj4-ie"
      },
      "outputs": [],
      "source": [
        "class LightGCN(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_dim, n_layers, reg, node_dropout, adj_mtx):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize Class attributes\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.emb_dim = emb_dim\n",
        "        self.l = adj_mtx\n",
        "        self.graph = self._convert_sp_mat_to_sp_tensor(self.l)\n",
        "\n",
        "        self.reg = reg\n",
        "        self.n_layers = n_layers\n",
        "        self.node_dropout = node_dropout\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weight_dict = self._init_weights()\n",
        "        print(\"Weights initialized.\")\n",
        "\n",
        "    # initialize weights\n",
        "    def _init_weights(self):\n",
        "        print(\"Initializing weights...\")\n",
        "        weight_dict = nn.ParameterDict()\n",
        "\n",
        "        initializer = torch.nn.init.xavier_uniform_\n",
        "        \n",
        "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
        "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
        "           \n",
        "        return weight_dict\n",
        "\n",
        "    # convert sparse matrix into sparse PyTorch tensor\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        \"\"\"\n",
        "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "        X = Adjacency matrix, scipy sparse matrix\n",
        "        \"\"\"\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
        "        v = torch.FloatTensor(coo.data)\n",
        "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
        "        return res\n",
        "\n",
        "    # apply node_dropout\n",
        "    def _droupout_sparse(self, X):\n",
        "        \"\"\"\n",
        "        Drop individual locations in X\n",
        "        \n",
        "        Arguments:\n",
        "        ---------\n",
        "        X = adjacency matrix (PyTorch sparse tensor)\n",
        "        dropout = fraction of nodes to drop\n",
        "        noise_shape = number of non non-zero entries of X\n",
        "        \"\"\"\n",
        "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
        "        i = X.coalesce().indices()\n",
        "        v = X.coalesce()._values()\n",
        "        i[:,node_dropout_mask] = 0\n",
        "        v[node_dropout_mask] = 0\n",
        "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
        "\n",
        "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        \"\"\"\n",
        "        Computes the forward pass\n",
        "        \n",
        "        Arguments:\n",
        "        ---------\n",
        "        u = user\n",
        "        i = positive item (user interacted with item)\n",
        "        j = negative item (user did not interact with item)\n",
        "        \"\"\"\n",
        "        # apply drop-out mask\n",
        "        graph = self._droupout_sparse(self.graph) if self.node_dropout > 0 else self.graph\n",
        "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
        "        final_embeddings = [ego_embeddings]\n",
        "\n",
        "        for k in range(self.n_layers):\n",
        "            ego_embeddings = torch.sparse.mm(graph, final_embeddings[k])\n",
        "            final_embeddings.append(ego_embeddings)                                       \n",
        "\n",
        "        final_embeddings = torch.stack(final_embeddings, dim=1)\n",
        "        final_embeddings = torch.mean(final_embeddings, dim=1)\n",
        "        \n",
        "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
        "\n",
        "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
        "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
        "        \n",
        "        # loss 계산\n",
        "        u_emb = u_final_embeddings[u] # user embeddings\n",
        "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
        "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
        "        \n",
        "        y_ui = torch.sum(torch.mul(u_emb, p_emb), dim = 1)                        \n",
        "        y_uj = torch.sum(torch.mul(u_emb, n_emb), dim = 1)\n",
        "        \n",
        "        log_prob = torch.mean(torch.log(torch.sigmoid(y_ui - y_uj))) \n",
        "        bpr_loss = -log_prob        \n",
        "        if self.reg > 0.:\n",
        "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
        "            l2reg = self.reg * l2norm\n",
        "            bpr_loss += l2reg\n",
        "\n",
        "        return bpr_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAjSLNX84-ie"
      },
      "source": [
        "# 4. 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH68CVsP4-if"
      },
      "outputs": [],
      "source": [
        "def train(model, make_graph_data_set, optimizer, n_batch):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for step in range(1, n_batch + 1):\n",
        "        user, pos, neg = make_graph_data_set.sampling()\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(user, pos, neg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_val += loss.item()\n",
        "    loss_val /= n_batch\n",
        "    return loss_val\n",
        "\n",
        "def split_matrix(X, n_splits=10):\n",
        "    splits = []\n",
        "    chunk_size = X.shape[0] // n_splits\n",
        "    for i in range(n_splits):\n",
        "        start = i * chunk_size\n",
        "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
        "        splits.append(X[start:end])\n",
        "    return splits\n",
        "\n",
        "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
        "    \n",
        "    r = (test_items * pred_items).gather(1, test_indices)\n",
        "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
        "    \n",
        "    dcg = (r[:, :k]/f).sum(1)                                               \n",
        "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
        "    ndcg = dcg/dcg_max                                                     \n",
        "    \n",
        "    ndcg[torch.isnan(ndcg)] = 0\n",
        "    return ndcg\n",
        "\n",
        "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
        "\n",
        "    # split matrices\n",
        "    ue_splits = split_matrix(u_emb)\n",
        "    tr_splits = split_matrix(Rtr)\n",
        "    te_splits = split_matrix(Rte)\n",
        "\n",
        "    recall_k, ndcg_k= [], []\n",
        "    # compute results for split matrices\n",
        "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
        "\n",
        "        scores = torch.mm(ue_f, i_emb.t())\n",
        "\n",
        "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
        "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
        "        scores = scores * non_train_items\n",
        "\n",
        "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
        "        \n",
        "        pred_items = torch.zeros_like(scores).float()\n",
        "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
        "\n",
        "        topk_preds = torch.zeros_like(scores).float()\n",
        "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
        "        \n",
        "        TP = (test_items * topk_preds).sum(1)                      \n",
        "        rec = TP/test_items.sum(1)\n",
        "   \n",
        "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
        "\n",
        "        recall_k.append(rec)\n",
        "        ndcg_k.append(ndcg)\n",
        "\n",
        "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m3hDCwe4-if"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZmsnxft4-if"
      },
      "outputs": [],
      "source": [
        "make_graph_data_set = MakeGraphDataSet(config = config)\n",
        "ngcf_adj_matrix = make_graph_data_set.get_ngcf_adj_matrix_data()\n",
        "R_train, R_valid, R_total = make_graph_data_set.get_R_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlhjKbVQ4-ig",
        "outputId": "2db4482d-4cd9-4e94-c80c-284b24a96175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing weights...\n",
            "Weights initialized.\n"
          ]
        }
      ],
      "source": [
        "model = LightGCN(\n",
        "    n_users = make_graph_data_set.num_user,\n",
        "    n_items = make_graph_data_set.num_item,\n",
        "    emb_dim = config.emb_dim,\n",
        "    n_layers = config.n_layers,\n",
        "    reg = config.reg,\n",
        "    node_dropout = config.node_dropout,\n",
        "    adj_mtx = ngcf_adj_matrix,\n",
        "    ).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YJ2Yba64-ig",
        "outputId": "f9303273-08ee-4fa8-a077-06406fd9addd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.69278| NDCG@10: 0.01350| HIT@10: 0.00283: 100%|██████████| 1/1 [00:05<00:00,  5.45s/it]\n",
            "Epoch:   2| Train loss: 0.69275| NDCG@10: 0.01927| HIT@10: 0.00417: 100%|██████████| 1/1 [00:05<00:00,  5.48s/it]\n",
            "Epoch:   3| Train loss: 0.69273| NDCG@10: 0.02203| HIT@10: 0.00492: 100%|██████████| 1/1 [00:05<00:00,  5.38s/it]\n",
            "Epoch:   4| Train loss: 0.69267| NDCG@10: 0.03238| HIT@10: 0.00730: 100%|██████████| 1/1 [00:05<00:00,  5.53s/it]\n",
            "Epoch:   5| Train loss: 0.69265| NDCG@10: 0.04731| HIT@10: 0.01118: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
            "Epoch:   6| Train loss: 0.69258| NDCG@10: 0.06658| HIT@10: 0.01654: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n",
            "Epoch:   7| Train loss: 0.69251| NDCG@10: 0.08573| HIT@10: 0.02221: 100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n",
            "Epoch:   8| Train loss: 0.69244| NDCG@10: 0.12460| HIT@10: 0.03294: 100%|██████████| 1/1 [00:05<00:00,  5.47s/it]\n",
            "Epoch:   9| Train loss: 0.69230| NDCG@10: 0.16138| HIT@10: 0.04426: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
            "Epoch:  10| Train loss: 0.69218| NDCG@10: 0.17931| HIT@10: 0.05067: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
            "Epoch:  11| Train loss: 0.69202| NDCG@10: 0.20119| HIT@10: 0.05693: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]\n",
            "Epoch:  12| Train loss: 0.69183| NDCG@10: 0.21406| HIT@10: 0.06289: 100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n",
            "Epoch:  13| Train loss: 0.69156| NDCG@10: 0.22638| HIT@10: 0.06811: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
            "Epoch:  14| Train loss: 0.69129| NDCG@10: 0.24754| HIT@10: 0.06915: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it]\n",
            "Epoch:  15| Train loss: 0.69091| NDCG@10: 0.24186| HIT@10: 0.06855: 100%|██████████| 1/1 [00:05<00:00,  5.16s/it]\n",
            "Epoch:  16| Train loss: 0.69054| NDCG@10: 0.25205| HIT@10: 0.07004: 100%|██████████| 1/1 [00:05<00:00,  5.39s/it]\n",
            "Epoch:  17| Train loss: 0.68999| NDCG@10: 0.26127| HIT@10: 0.07288: 100%|██████████| 1/1 [00:05<00:00,  5.74s/it]\n",
            "Epoch:  18| Train loss: 0.68949| NDCG@10: 0.25142| HIT@10: 0.07124: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
            "Epoch:  19| Train loss: 0.68873| NDCG@10: 0.25004| HIT@10: 0.06975: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n",
            "Epoch:  20| Train loss: 0.68798| NDCG@10: 0.25374| HIT@10: 0.06945: 100%|██████████| 1/1 [00:05<00:00,  5.28s/it]\n",
            "Epoch:  21| Train loss: 0.68709| NDCG@10: 0.24410| HIT@10: 0.06990: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n",
            "Epoch:  22| Train loss: 0.68611| NDCG@10: 0.25080| HIT@10: 0.06915: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n",
            "Epoch:  23| Train loss: 0.68485| NDCG@10: 0.24898| HIT@10: 0.06945: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
            "Epoch:  24| Train loss: 0.68357| NDCG@10: 0.25427| HIT@10: 0.07019: 100%|██████████| 1/1 [00:05<00:00,  5.44s/it]\n",
            "Epoch:  25| Train loss: 0.68213| NDCG@10: 0.25507| HIT@10: 0.06915: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
            "Epoch:  26| Train loss: 0.68050| NDCG@10: 0.24889| HIT@10: 0.06751: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
            "Epoch:  27| Train loss: 0.67889| NDCG@10: 0.24633| HIT@10: 0.06736: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n",
            "Epoch:  28| Train loss: 0.67692| NDCG@10: 0.24964| HIT@10: 0.06721: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n",
            "Epoch:  29| Train loss: 0.67511| NDCG@10: 0.24572| HIT@10: 0.06468: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
            "Epoch:  30| Train loss: 0.67292| NDCG@10: 0.24339| HIT@10: 0.06692: 100%|██████████| 1/1 [00:05<00:00,  5.44s/it]\n",
            "Epoch:  31| Train loss: 0.67017| NDCG@10: 0.24776| HIT@10: 0.06677: 100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n",
            "Epoch:  32| Train loss: 0.66777| NDCG@10: 0.23870| HIT@10: 0.06736: 100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n",
            "Epoch:  33| Train loss: 0.66542| NDCG@10: 0.24410| HIT@10: 0.06811: 100%|██████████| 1/1 [00:05<00:00,  5.21s/it]\n",
            "Epoch:  34| Train loss: 0.66297| NDCG@10: 0.24875| HIT@10: 0.06826: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n",
            "Epoch:  35| Train loss: 0.65962| NDCG@10: 0.24625| HIT@10: 0.06453: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
            "Epoch:  36| Train loss: 0.65664| NDCG@10: 0.23234| HIT@10: 0.06274: 100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n",
            "Epoch:  37| Train loss: 0.65298| NDCG@10: 0.24306| HIT@10: 0.06468: 100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
            "Epoch:  38| Train loss: 0.64947| NDCG@10: 0.24800| HIT@10: 0.06528: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
            "Epoch:  39| Train loss: 0.64612| NDCG@10: 0.24549| HIT@10: 0.06483: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n",
            "Epoch:  40| Train loss: 0.64229| NDCG@10: 0.24250| HIT@10: 0.06677: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it]\n",
            "Epoch:  41| Train loss: 0.63882| NDCG@10: 0.24562| HIT@10: 0.06230: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n",
            "Epoch:  42| Train loss: 0.63442| NDCG@10: 0.24278| HIT@10: 0.06289: 100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n",
            "Epoch:  43| Train loss: 0.63190| NDCG@10: 0.23310| HIT@10: 0.06632: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]\n",
            "Epoch:  44| Train loss: 0.62759| NDCG@10: 0.24890| HIT@10: 0.06468: 100%|██████████| 1/1 [00:05<00:00,  5.28s/it]\n",
            "Epoch:  45| Train loss: 0.62341| NDCG@10: 0.24873| HIT@10: 0.06557: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n",
            "Epoch:  46| Train loss: 0.62009| NDCG@10: 0.24461| HIT@10: 0.06408: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
            "Epoch:  47| Train loss: 0.61624| NDCG@10: 0.24507| HIT@10: 0.06304: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]\n",
            "Epoch:  48| Train loss: 0.61030| NDCG@10: 0.24854| HIT@10: 0.06408: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
            "Epoch:  49| Train loss: 0.60646| NDCG@10: 0.24108| HIT@10: 0.06349: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it]\n",
            "Epoch:  50| Train loss: 0.60212| NDCG@10: 0.24147| HIT@10: 0.06542: 100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n"
          ]
        }
      ],
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            make_graph_data_set = make_graph_data_set, \n",
        "            optimizer = optimizer,\n",
        "            n_batch = config.n_batch,\n",
        "            )\n",
        "        with torch.no_grad():\n",
        "            ndcg, hit = evaluate(\n",
        "                u_emb = model.u_final_embeddings.detach(), \n",
        "                i_emb = model.i_final_embeddings.detach(), \n",
        "                Rtr = R_train, \n",
        "                Rte = R_valid, \n",
        "                k = 10,\n",
        "                )\n",
        "\n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    }
  ]
}