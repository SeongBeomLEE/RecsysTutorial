{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecVAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1evrCGpMqJ5p9riyjuK5gz4EIiOyfixc1",
      "authorship_tag": "ABX9TyPM3QPpy4d2rTojqCpQQ3Sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/RecVAE/RecVAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzJgxsrewLBy",
        "outputId": "46b2042c-925e-47b2-d5af-68004d1e9e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-box\n",
            "  Downloading python_box-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug-br-pPu9vZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "from copy import deepcopy\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "torch.set_printoptions(sci_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbRKDSg4u9vc"
      },
      "source": [
        "# 1. 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlm1UrKvoC_O"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n",
        "\n",
        "    'model_path' : \"/content/drive/MyDrive/RecsysTutorial/model\", # 모델 저장 경로\n",
        "    'model_name' : 'RecVAE.pt',\n",
        "\n",
        "    'hidden_dim': 600,\n",
        "    'latent_dim' : 200,\n",
        "    'dropout_rate' : 0.5,\n",
        "    'gamma' : 0.005,\n",
        "    'beta' : None,\n",
        "    'not_alternating' :True,\n",
        "    'e_num_epochs' : 3,\n",
        "    'd_num_epochs' : 1,\n",
        "\n",
        "    'lr' : 5e-4,\n",
        "    'batch_size' : 128,\n",
        "    'num_epochs' : 50,\n",
        "    'num_workers' : 2,\n",
        "\n",
        "    'valid_samples' : 10,\n",
        "    'seed' : 22,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqVUlLd9u9ve"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(config.model_path):\n",
        "    os.mkdir(config.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjDxy0fJu9vf"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W64BYWl0u9vg"
      },
      "outputs": [],
      "source": [
        "class MakeMatrixDataSet():\n",
        "    \"\"\"\n",
        "    MatrixDataSet 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n",
        "        \n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder[x])\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
        "            users[user].append(item)\n",
        "        \n",
        "        for user in users:\n",
        "            np.random.seed(self.config.seed)\n",
        "\n",
        "            user_total = users[user]\n",
        "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
        "            train = list(set(user_total) - set(valid))\n",
        "\n",
        "            user_train[user] = train\n",
        "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
        "\n",
        "        return user_train, user_valid\n",
        "    \n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid\n",
        "\n",
        "    def make_matrix(self, user_list, train = True):\n",
        "        \"\"\"\n",
        "        user_item_dict를 바탕으로 행렬 생성\n",
        "        \"\"\"\n",
        "        mat = torch.zeros(size = (user_list.size(0), self.num_item))\n",
        "        for idx, user in enumerate(user_list):\n",
        "            if train:\n",
        "                mat[idx, self.user_train[user.item()]] = 1\n",
        "            else:\n",
        "                mat[idx, self.user_train[user.item()] + self.user_valid[user.item()]] = 1\n",
        "        return mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IldCGmY8u9vh"
      },
      "outputs": [],
      "source": [
        "class AEDataSet(Dataset):\n",
        "    def __init__(self, num_user):\n",
        "        self.num_user = num_user\n",
        "        self.users = [i for i in range(num_user)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_user\n",
        "\n",
        "    def __getitem__(self, idx): \n",
        "        user = self.users[idx]\n",
        "        return torch.LongTensor([user])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtf1I824nx5V"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    return x.mul(torch.sigmoid(x))\n",
        "\n",
        "def log_norm_pdf(x, mu, logvar):\n",
        "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
        "\n",
        "class CompositePrior(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
        "        super(CompositePrior, self).__init__()\n",
        "        \n",
        "        self.mixture_weights = mixture_weights\n",
        "        \n",
        "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.mu_prior.data.fill_(0)\n",
        "        \n",
        "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_prior.data.fill_(0)\n",
        "        \n",
        "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_uniform_prior.data.fill_(10)\n",
        "        \n",
        "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.encoder_old.requires_grad_(False)\n",
        "        \n",
        "    def forward(self, x, z):\n",
        "\n",
        "        post_mu, post_logvar = self.encoder_old(x, dropout_rate = 0)\n",
        "\n",
        "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
        "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
        "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
        "        \n",
        "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
        "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
        "\n",
        "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
        "\n",
        "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
        "\n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        \n",
        "    def forward(self, x, dropout_rate):\n",
        "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
        "        x = x / norm[:, None]\n",
        "    \n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "        \n",
        "        h1 = self.ln1(swish(self.fc1(x)))\n",
        "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
        "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
        "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
        "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
        "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
        "\n",
        "\n",
        "class RecVAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim = 600, latent_dim = 200):\n",
        "        super(RecVAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
        "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
        "        \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5*logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, user_ratings, beta=None, gamma=0.005, dropout_rate=0.5, calculate_loss=True):\n",
        "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)    \n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_pred = self.decoder(z)\n",
        "\n",
        "        if calculate_loss:\n",
        "            if gamma:\n",
        "                norm = user_ratings.sum(dim=-1)\n",
        "                kl_weight = gamma * norm\n",
        "            elif beta:\n",
        "                kl_weight = beta\n",
        "\n",
        "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
        "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
        "            negative_elbo = -(mll - kld)\n",
        "            \n",
        "            return (mll, kld), negative_elbo\n",
        "            \n",
        "        else:\n",
        "            return x_pred\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
      ],
      "metadata": {
        "id": "UFFzRy9qUEG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk-bL5p4nx5W"
      },
      "source": [
        "# 4. 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmooa3n1u9vj"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, data_loader, make_matrix_data_set, beta, gamma, dropout_rate):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for users in data_loader:\n",
        "        mat = make_matrix_data_set.make_matrix(users)\n",
        "        mat = mat.to(device)\n",
        "        _, loss = model(user_ratings = mat, beta = beta, gamma = gamma, dropout_rate = dropout_rate)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_val += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def get_ndcg(pred_list, true_list):\n",
        "    ndcg = 0\n",
        "    for rank, pred in enumerate(pred_list):\n",
        "        if pred in true_list:\n",
        "            ndcg += 1 / np.log2(rank + 2)\n",
        "    return ndcg\n",
        "\n",
        "# hit == recall == precision\n",
        "def get_hit(pred_list, true_list):\n",
        "    hit_list = set(true_list) & set(pred_list)\n",
        "    hit = len(hit_list) / len(true_list)\n",
        "    return hit\n",
        "\n",
        "def evaluate(model, data_loader, user_train, user_valid, make_matrix_data_set):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for users in data_loader:\n",
        "            mat = make_matrix_data_set.make_matrix(users)\n",
        "            mat = mat.to(device)\n",
        "\n",
        "            recon_mat = model(mat, calculate_loss = False)\n",
        "            recon_mat = recon_mat.softmax(dim = 1)\n",
        "            recon_mat[mat == 1] = -1.\n",
        "            rec_list = recon_mat.argsort(dim = 1)\n",
        "\n",
        "            for user, rec in zip(users, rec_list):\n",
        "                uv = user_valid[user.item()]\n",
        "                up = rec[-10:].cpu().numpy().tolist()\n",
        "                NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
        "                HIT += get_hit(pred_list = up, true_list = uv)\n",
        "\n",
        "    NDCG /= len(data_loader.dataset)\n",
        "    HIT /= len(data_loader.dataset)\n",
        "\n",
        "    return NDCG, HIT\n",
        "\n",
        "def predict(model, data_loader, user_train, user_valid, make_matrix_data_set):\n",
        "    model.eval()\n",
        "    \n",
        "    user2rec_list = {}\n",
        "    with torch.no_grad():\n",
        "        for users in data_loader:\n",
        "            mat = make_matrix_data_set.make_matrix(users, train = False)\n",
        "            mat = mat.to(device)\n",
        "\n",
        "            recon_mat = model(mat, calculate_loss = False)\n",
        "            recon_mat = recon_mat.softmax(dim = 1)\n",
        "            recon_mat[mat == 1] = -1.\n",
        "            rec_list = recon_mat.argsort(dim = 1)\n",
        "\n",
        "            for user, rec in zip(users, rec_list):\n",
        "                up = rec[-10:].cpu().numpy().tolist()\n",
        "                user2rec_list[user.item()] = up\n",
        "    \n",
        "    return user2rec_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozBvgarCnx5W"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zXVEf6fu9vk"
      },
      "outputs": [],
      "source": [
        "make_matrix_data_set = MakeMatrixDataSet(config = config)\n",
        "user_train, user_valid = make_matrix_data_set.get_train_valid_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hGPyH54u9vk"
      },
      "outputs": [],
      "source": [
        "ae_dataset = AEDataSet(\n",
        "    num_user = make_matrix_data_set.num_user,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyL3vriiu9vl"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(\n",
        "    ae_dataset,\n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    pin_memory = True,\n",
        "    num_workers = config.num_workers,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpiWnBV8u9vm"
      },
      "outputs": [],
      "source": [
        "model = RecVAE(\n",
        "    input_dim = make_matrix_data_set.num_item,\n",
        "    hidden_dim = config.hidden_dim,\n",
        "    latent_dim = config.latent_dim).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "optimizer_encoder = torch.optim.Adam(model.encoder.parameters(), lr=5e-4)\n",
        "optimizer_decoder = torch.optim.Adam(model.decoder.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        \n",
        "        if config.not_alternating:\n",
        "            train_loss = train(\n",
        "                    model = model,\n",
        "                    optimizer = optimizer, \n",
        "                    data_loader = data_loader,\n",
        "                    make_matrix_data_set = make_matrix_data_set,\n",
        "                    beta = config.beta,\n",
        "                    gamma = config.gamma, \n",
        "                    dropout_rate = config.dropout_rate,\n",
        "                    )\n",
        "        \n",
        "        else:\n",
        "            for _ in range(config.e_num_epochs):\n",
        "                train_loss = train(\n",
        "                        model = model,\n",
        "                        optimizer = optimizer_encoder, \n",
        "                        data_loader = data_loader,\n",
        "                        make_matrix_data_set = make_matrix_data_set,\n",
        "                        beta = config.beta,\n",
        "                        gamma = config.gamma, \n",
        "                        dropout_rate = config.dropout_rate,\n",
        "                        )\n",
        "\n",
        "            model.update_prior()\n",
        "            \n",
        "            for _ in range(config.d_num_epochs):\n",
        "                train_loss = train(\n",
        "                        model = model,\n",
        "                        optimizer = optimizer_decoder, \n",
        "                        data_loader = data_loader,\n",
        "                        make_matrix_data_set = make_matrix_data_set,\n",
        "                        beta = config.beta,\n",
        "                        gamma = config.gamma, \n",
        "                        dropout_rate = 0.0,\n",
        "                        )\n",
        "\n",
        "        ndcg, hit = evaluate(\n",
        "            model = model,\n",
        "            data_loader = data_loader,\n",
        "            user_train = user_train,\n",
        "            user_valid = user_valid,\n",
        "            make_matrix_data_set = make_matrix_data_set,\n",
        "            )\n",
        "\n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE6KYgPAuR7p",
        "outputId": "a208c9c4-b6ec-4280-f9c5-ac1383871ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 1366.43835| NDCG@10: 0.05107| HIT@10: 0.01282: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
            "Epoch:   2| Train loss: 1295.67975| NDCG@10: 0.11427| HIT@10: 0.02578: 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
            "Epoch:   3| Train loss: 1277.01697| NDCG@10: 0.16367| HIT@10: 0.03681: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
            "Epoch:   4| Train loss: 1285.28323| NDCG@10: 0.18803| HIT@10: 0.04605: 100%|██████████| 1/1 [00:03<00:00,  3.64s/it]\n",
            "Epoch:   5| Train loss: 1242.10782| NDCG@10: 0.23129| HIT@10: 0.05544: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
            "Epoch:   6| Train loss: 1256.72735| NDCG@10: 0.24675| HIT@10: 0.05991: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
            "Epoch:   7| Train loss: 1131.88567| NDCG@10: 0.29642| HIT@10: 0.06706: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
            "Epoch:   8| Train loss: 1128.96410| NDCG@10: 0.31920| HIT@10: 0.07526: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
            "Epoch:   9| Train loss: 1313.60253| NDCG@10: 0.35016| HIT@10: 0.08286: 100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n",
            "Epoch:  10| Train loss: 1155.99115| NDCG@10: 0.36690| HIT@10: 0.08629: 100%|██████████| 1/1 [00:05<00:00,  5.32s/it]\n",
            "Epoch:  11| Train loss: 1134.92666| NDCG@10: 0.39249| HIT@10: 0.09016: 100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n",
            "Epoch:  12| Train loss: 1136.12503| NDCG@10: 0.42768| HIT@10: 0.09732: 100%|██████████| 1/1 [00:05<00:00,  5.17s/it]\n",
            "Epoch:  13| Train loss: 1115.52193| NDCG@10: 0.45602| HIT@10: 0.10760: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
            "Epoch:  14| Train loss: 1172.24092| NDCG@10: 0.43114| HIT@10: 0.10462: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
            "Epoch:  15| Train loss: 1142.85889| NDCG@10: 0.45524| HIT@10: 0.11058: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
            "Epoch:  16| Train loss: 1163.55892| NDCG@10: 0.45544| HIT@10: 0.10954: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
            "Epoch:  17| Train loss: 1125.87046| NDCG@10: 0.49154| HIT@10: 0.11550: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
            "Epoch:  18| Train loss: 1200.35089| NDCG@10: 0.49383| HIT@10: 0.12012: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
            "Epoch:  19| Train loss: 1147.22356| NDCG@10: 0.50988| HIT@10: 0.12206: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
            "Epoch:  20| Train loss: 1117.27836| NDCG@10: 0.50438| HIT@10: 0.12578: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
            "Epoch:  21| Train loss: 1082.21211| NDCG@10: 0.50670| HIT@10: 0.12757: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n",
            "Epoch:  22| Train loss: 1067.10485| NDCG@10: 0.52780| HIT@10: 0.12951: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
            "Epoch:  23| Train loss: 1106.44068| NDCG@10: 0.55498| HIT@10: 0.13219: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
            "Epoch:  24| Train loss: 1130.60174| NDCG@10: 0.56763| HIT@10: 0.13383: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
            "Epoch:  25| Train loss: 1117.11369| NDCG@10: 0.56007| HIT@10: 0.13249: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
            "Epoch:  26| Train loss: 1079.99589| NDCG@10: 0.57975| HIT@10: 0.13875: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
            "Epoch:  27| Train loss: 1129.37262| NDCG@10: 0.56595| HIT@10: 0.13756: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
            "Epoch:  28| Train loss: 1119.54439| NDCG@10: 0.55462| HIT@10: 0.13651: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
            "Epoch:  29| Train loss: 1078.68546| NDCG@10: 0.57466| HIT@10: 0.13905: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
            "Epoch:  30| Train loss: 1108.09867| NDCG@10: 0.58760| HIT@10: 0.14173: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
            "Epoch:  31| Train loss: 1135.71531| NDCG@10: 0.58827| HIT@10: 0.14203: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
            "Epoch:  32| Train loss: 1086.27512| NDCG@10: 0.57984| HIT@10: 0.14277: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
            "Epoch:  33| Train loss: 1068.88895| NDCG@10: 0.58274| HIT@10: 0.14069: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n",
            "Epoch:  34| Train loss: 1142.65207| NDCG@10: 0.56992| HIT@10: 0.14113: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
            "Epoch:  35| Train loss: 1065.78297| NDCG@10: 0.60308| HIT@10: 0.14694: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
            "Epoch:  36| Train loss: 1058.07468| NDCG@10: 0.58891| HIT@10: 0.14292: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
            "Epoch:  37| Train loss: 1033.65404| NDCG@10: 0.60388| HIT@10: 0.14888: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
            "Epoch:  38| Train loss: 1053.23622| NDCG@10: 0.59525| HIT@10: 0.14709: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
            "Epoch:  39| Train loss: 1083.83191| NDCG@10: 0.61659| HIT@10: 0.14873: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
            "Epoch:  40| Train loss: 1165.16069| NDCG@10: 0.61278| HIT@10: 0.14784: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
            "Epoch:  41| Train loss: 1081.05700| NDCG@10: 0.63759| HIT@10: 0.15440: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n",
            "Epoch:  42| Train loss: 1179.56849| NDCG@10: 0.61446| HIT@10: 0.15082: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n",
            "Epoch:  43| Train loss: 1058.66041| NDCG@10: 0.61383| HIT@10: 0.14799: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n",
            "Epoch:  44| Train loss: 1110.82767| NDCG@10: 0.62327| HIT@10: 0.15082: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
            "Epoch:  45| Train loss: 1111.53104| NDCG@10: 0.64438| HIT@10: 0.15544: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n",
            "Epoch:  46| Train loss: 1020.36043| NDCG@10: 0.64624| HIT@10: 0.15380: 100%|██████████| 1/1 [00:03<00:00,  3.26s/it]\n",
            "Epoch:  47| Train loss: 1110.36015| NDCG@10: 0.62420| HIT@10: 0.15276: 100%|██████████| 1/1 [00:02<00:00,  2.97s/it]\n",
            "Epoch:  48| Train loss: 1042.06358| NDCG@10: 0.63647| HIT@10: 0.15604: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
            "Epoch:  49| Train loss: 1037.68335| NDCG@10: 0.65228| HIT@10: 0.15350: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
            "Epoch:  50| Train loss: 1048.30044| NDCG@10: 0.62095| HIT@10: 0.15097: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n"
          ]
        }
      ]
    }
  ]
}