{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGCF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EA8nZ4cJ7XsnERyeV8cO_NrsamvOL-3e",
      "authorship_tag": "ABX9TyNzhOfyYKgGB/uywhLnCxeS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/NGCF/NGCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjOnv1NS52ww",
        "outputId": "2112e7f1-54c1-402a-a12f-8afcb5bac7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-box\n",
            "  Downloading python_box-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 10.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtBCq174_N1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8uF4zk4_N2"
      },
      "source": [
        "# 1. 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7_2AsnT4_N2"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n",
        "\n",
        "    'model_path' : \"/content/drive/MyDrive/RecsysTutorial/model\", # 모델 저장 경로\n",
        "    'model_name' : 'NGCF.pt',\n",
        "\n",
        "    'num_epochs' : 50,\n",
        "    \"reg\" : 1e-5,\n",
        "    'lr' : 0.0001,\n",
        "    \"emb_dim\" : 128,\n",
        "    \"layers\" : [128, 128],\n",
        "    'batch_size' : 500,\n",
        "    \"node_dropout\" : 0.2,\n",
        "    \"mess_dropout\" : 0.2,\n",
        "\n",
        "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
        "    'seed' : 22,\n",
        "    'n_batch' : 10,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1RD4CH4_N3"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(config.model_path):\n",
        "    os.mkdir(config.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlRF8eUo4_N3"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXbxctF04_N3"
      },
      "outputs": [],
      "source": [
        "class MakeGraphDataSet():\n",
        "    \"\"\"\n",
        "    GraphDataSet 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder[x])\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        \n",
        "        self.exist_users = [i for i in range(self.num_user)]\n",
        "        self.exist_items = [i for i in range(self.num_item)]\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "        self.R_train, self.R_valid, self.R_total = self.generate_dok_matrix()\n",
        "        self.ngcf_adj_matrix = self.generate_ngcf_adj_matrix()\n",
        "        self.n_train = len(self.R_train)\n",
        "        self.batch_size = self.config.batch_size\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
        "            users[user].append(item)\n",
        "        \n",
        "        for user in users:\n",
        "            np.random.seed(self.config.seed)\n",
        "\n",
        "            user_total = users[user]\n",
        "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
        "            train = list(set(user_total) - set(valid))\n",
        "\n",
        "            user_train[user] = train\n",
        "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
        "\n",
        "        return user_train, user_valid\n",
        "    \n",
        "    def generate_dok_matrix(self):\n",
        "        R_train = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        R_valid = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        R_total = sp.dok_matrix((self.num_user, self.num_item), dtype=np.float32)\n",
        "        user_list = self.exist_users\n",
        "        for user in user_list:\n",
        "            train_items = self.user_train[user]\n",
        "            valid_items = self.user_valid[user]\n",
        "            \n",
        "            for train_item in train_items:\n",
        "                R_train[user, train_item] = 1.0\n",
        "                R_total[user, train_item] = 1.0\n",
        "            \n",
        "            for valid_item in valid_items:\n",
        "                R_valid[user, valid_item] = 1.0\n",
        "                R_total[user, valid_item] = 1.0\n",
        "        \n",
        "        return R_train, R_valid, R_total\n",
        "\n",
        "    def generate_ngcf_adj_matrix(self):\n",
        "        adj_mat = sp.dok_matrix((self.num_user + self.num_item, self.num_user + self.num_item), dtype=np.float32)\n",
        "        adj_mat = adj_mat.tolil() # to_list\n",
        "        R = self.R_train.tolil()\n",
        "\n",
        "        adj_mat[:self.num_user, self.num_user:] = R\n",
        "        adj_mat[self.num_user:, :self.num_user] = R.T\n",
        "        adj_mat = adj_mat.todok() # to_dok_matrix\n",
        "\n",
        "        def normalized_adj_single(adj):\n",
        "            rowsum = np.array(adj.sum(1))\n",
        "            d_inv = np.power(rowsum, -.5).flatten()  \n",
        "            d_inv[np.isinf(d_inv)] = 0.\n",
        "            d_mat_inv = sp.diags(d_inv)\n",
        "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv)\n",
        "\n",
        "            return norm_adj.tocoo()\n",
        "\n",
        "        ngcf_adj_matrix = normalized_adj_single(adj_mat)\n",
        "        return ngcf_adj_matrix.tocsr()\n",
        "\n",
        "    def sampling(self):\n",
        "        users = random.sample(self.exist_users, self.config.batch_size)\n",
        "\n",
        "        def sample_pos_items_for_u(u, num):\n",
        "            pos_items = self.user_train[u]\n",
        "            pos_batch = random.sample(pos_items, num)\n",
        "            return pos_batch\n",
        "        \n",
        "        def sample_neg_items_for_u(u, num):\n",
        "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
        "            neg_batch = random.sample(neg_items, num)\n",
        "            return neg_batch\n",
        "        \n",
        "        pos_items, neg_items = [], []\n",
        "        for user in users:\n",
        "            pos_items += sample_pos_items_for_u(user, 1)\n",
        "            neg_items += sample_neg_items_for_u(user, 1)\n",
        "        \n",
        "        return users, pos_items, neg_items\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid\n",
        "\n",
        "    def get_R_data(self):\n",
        "        return self.R_train, self.R_valid, self.R_total\n",
        "\n",
        "    def get_ngcf_adj_matrix_data(self):\n",
        "        return self.ngcf_adj_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4_9-bJ4_N4"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21kbA_V04_N4"
      },
      "outputs": [],
      "source": [
        "class NGCF(nn.Module):\n",
        "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout, adj_mtx):\n",
        "        super().__init__()\n",
        "\n",
        "        # initialize Class attributes\n",
        "        self.n_users = n_users\n",
        "        self.n_items = n_items\n",
        "        self.emb_dim = emb_dim\n",
        "        self.l_matrix = adj_mtx\n",
        "        self.l_plus_i_matrix = adj_mtx + sp.eye(adj_mtx.shape[0])\n",
        "        self.reg = reg\n",
        "        self.layers = layers\n",
        "        self.n_layers = len(self.layers)\n",
        "        self.node_dropout = node_dropout\n",
        "        self.mess_dropout = mess_dropout\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weight_dict = self._init_weights()\n",
        "        print(\"Weights initialized.\")\n",
        "\n",
        "        # Create Matrix 'L+I', PyTorch sparse tensor of SP adjacency_mtx\n",
        "        self.L_plus_I = self._convert_sp_mat_to_sp_tensor(self.l_plus_i_matrix)\n",
        "        self.L = self._convert_sp_mat_to_sp_tensor(self.l_matrix)\n",
        "\n",
        "    # initialize weights\n",
        "    def _init_weights(self):\n",
        "        print(\"Initializing weights...\")\n",
        "        weight_dict = nn.ParameterDict()\n",
        "\n",
        "        initializer = torch.nn.init.xavier_uniform_\n",
        "        \n",
        "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
        "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
        "\n",
        "        weight_size_list = [self.emb_dim] + self.layers\n",
        "\n",
        "        for k in range(self.n_layers):\n",
        "            weight_dict['W_one_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
        "            weight_dict['b_one_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
        "            \n",
        "            weight_dict['W_two_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
        "            weight_dict['b_two_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
        "           \n",
        "        return weight_dict\n",
        "\n",
        "    # convert sparse matrix into sparse PyTorch tensor\n",
        "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
        "        \"\"\"\n",
        "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
        "\n",
        "        Arguments:\n",
        "        ----------\n",
        "        X = Adjacency matrix, scipy sparse matrix\n",
        "        \"\"\"\n",
        "        coo = X.tocoo().astype(np.float32)\n",
        "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
        "        v = torch.FloatTensor(coo.data)\n",
        "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
        "        return res\n",
        "\n",
        "    # apply node_dropout\n",
        "    def _droupout_sparse(self, X):\n",
        "        \"\"\"\n",
        "        Drop individual locations in X\n",
        "        \n",
        "        Arguments:\n",
        "        ---------\n",
        "        X = adjacency matrix (PyTorch sparse tensor)\n",
        "        dropout = fraction of nodes to drop\n",
        "        noise_shape = number of non non-zero entries of X\n",
        "        \"\"\"\n",
        "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
        "        i = X.coalesce().indices()\n",
        "        v = X.coalesce()._values()\n",
        "        i[:,node_dropout_mask] = 0\n",
        "        v[node_dropout_mask] = 0\n",
        "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
        "\n",
        "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
        "\n",
        "    def forward(self, u, i, j):\n",
        "        \"\"\"\n",
        "        Computes the forward pass\n",
        "        \n",
        "        Arguments:\n",
        "        ---------\n",
        "        u = user\n",
        "        i = positive item (user interacted with item)\n",
        "        j = negative item (user did not interact with item)\n",
        "        \"\"\"\n",
        "        # apply drop-out mask\n",
        "        L_plus_I_hat = self._droupout_sparse(self.L_plus_I) if self.node_dropout > 0 else self.L_plus_I\n",
        "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
        "        \n",
        "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
        "\n",
        "        final_embeddings = [ego_embeddings]\n",
        "\n",
        "        for k in range(self.n_layers):\n",
        "            \n",
        "            # (L+I)E\n",
        "            side_L_plus_I_embeddings = torch.sparse.mm(L_plus_I_hat, final_embeddings[k])\n",
        "            \n",
        "            # (L+I)EW_1 + b_1\n",
        "            simple_embeddings = torch.matmul(side_L_plus_I_embeddings, self.weight_dict['W_one_%d' % k]) + self.weight_dict['b_one_%d' % k]\n",
        "            \n",
        "            # LE\n",
        "            side_L_embeddings = torch.sparse.mm(L_hat, final_embeddings[k])                              \n",
        "            \n",
        "            # LEE\n",
        "            interaction_embeddings = torch.mul(side_L_embeddings, final_embeddings[k])\n",
        "                                             \n",
        "            # LEEW_2 + b_2\n",
        "            interaction_embeddings = torch.matmul(interaction_embeddings, self.weight_dict['W_two_%d' % k]) + self.weight_dict['b_two_%d' % k]\n",
        "\n",
        "            # non-linear activation \n",
        "            ego_embeddings =  F.leaky_relu(simple_embeddings + interaction_embeddings)\n",
        "            \n",
        "            # add message dropout\n",
        "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
        "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
        "\n",
        "            # Perform L2 normalization\n",
        "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
        "            \n",
        "            final_embeddings.append(norm_embeddings)                                            \n",
        "\n",
        "        \n",
        "        final_embeddings = torch.cat(final_embeddings, 1)                           \n",
        "    \n",
        "        # back to user/item dimension\n",
        "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
        "\n",
        "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
        "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
        "        \n",
        "        u_emb = u_final_embeddings[u] # user embeddings\n",
        "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
        "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
        "    \n",
        "        y_ui = torch.sum(torch.mul(u_emb, p_emb), dim = 1)                           \n",
        "        y_uj = torch.sum(torch.mul(u_emb, n_emb), dim = 1)\n",
        "        \n",
        "        log_prob = torch.mean(torch.log(torch.sigmoid(y_ui - y_uj)))\n",
        "        bpr_loss = -log_prob        \n",
        "        if self.reg > 0.:\n",
        "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
        "            l2reg = self.reg * l2norm\n",
        "            bpr_loss += l2reg\n",
        "        \n",
        "        return bpr_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9vAyOS14_N5"
      },
      "source": [
        "# 4. 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8_nloUG4_N5"
      },
      "outputs": [],
      "source": [
        "def train(model, make_graph_data_set, optimizer, n_batch):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "    for step in range(1, n_batch + 1):\n",
        "        user, pos, neg = make_graph_data_set.sampling()\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(user, pos, neg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_val += loss.item()\n",
        "    loss_val /= n_batch\n",
        "    return loss_val\n",
        "\n",
        "def split_matrix(X, n_splits=10):\n",
        "    splits = []\n",
        "    chunk_size = X.shape[0] // n_splits\n",
        "    for i in range(n_splits):\n",
        "        start = i * chunk_size\n",
        "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
        "        splits.append(X[start:end])\n",
        "    return splits\n",
        "\n",
        "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
        "    \n",
        "    r = (test_items * pred_items).gather(1, test_indices)\n",
        "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
        "    \n",
        "    dcg = (r[:, :k]/f).sum(1)                                               \n",
        "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
        "    ndcg = dcg/dcg_max                                                     \n",
        "    \n",
        "    ndcg[torch.isnan(ndcg)] = 0\n",
        "    return ndcg\n",
        "\n",
        "def evaluate(u_emb, i_emb, Rtr, Rte, k = 10):\n",
        "\n",
        "    # split matrices\n",
        "    ue_splits = split_matrix(u_emb)\n",
        "    tr_splits = split_matrix(Rtr)\n",
        "    te_splits = split_matrix(Rte)\n",
        "\n",
        "    recall_k, ndcg_k= [], []\n",
        "    # compute results for split matrices\n",
        "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
        "\n",
        "        scores = torch.mm(ue_f, i_emb.t())\n",
        "\n",
        "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
        "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
        "        scores = scores * non_train_items\n",
        "\n",
        "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
        "        \n",
        "        pred_items = torch.zeros_like(scores).float()\n",
        "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
        "\n",
        "        topk_preds = torch.zeros_like(scores).float()\n",
        "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
        "        \n",
        "        TP = (test_items * topk_preds).sum(1)                      \n",
        "        rec = TP/test_items.sum(1)\n",
        "   \n",
        "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
        "\n",
        "        recall_k.append(rec)\n",
        "        ndcg_k.append(ndcg)\n",
        "\n",
        "    return torch.cat(ndcg_k).mean(), torch.cat(recall_k).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtixKYhl4_N6"
      },
      "source": [
        "# 5. 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tYssWww4_N6"
      },
      "outputs": [],
      "source": [
        "make_graph_data_set = MakeGraphDataSet(config = config)\n",
        "ngcf_adj_matrix = make_graph_data_set.get_ngcf_adj_matrix_data()\n",
        "R_train, R_valid, R_total = make_graph_data_set.get_R_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-bTkN_G4_N6",
        "outputId": "48f9f286-cdae-4766-bcbf-836f4b8a3c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing weights...\n",
            "Weights initialized.\n"
          ]
        }
      ],
      "source": [
        "model = NGCF(\n",
        "    n_users = make_graph_data_set.num_user,\n",
        "    n_items = make_graph_data_set.num_item,\n",
        "    emb_dim = config.emb_dim,\n",
        "    layers = config.layers,\n",
        "    reg = config.reg,\n",
        "    node_dropout = config.node_dropout,\n",
        "    mess_dropout = config.mess_dropout,\n",
        "    adj_mtx = ngcf_adj_matrix,\n",
        "    ).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5agnZZU4_N6",
        "outputId": "4ded8177-2d92-4a4c-be3f-e27c533005cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.68279| NDCG@10: 0.01315| HIT@10: 0.00298: 100%|██████████| 1/1 [00:09<00:00,  9.85s/it]\n",
            "Epoch:   2| Train loss: 0.68200| NDCG@10: 0.01403| HIT@10: 0.00343: 100%|██████████| 1/1 [00:12<00:00, 12.41s/it]\n",
            "Epoch:   3| Train loss: 0.68012| NDCG@10: 0.02026| HIT@10: 0.00417: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]\n",
            "Epoch:   4| Train loss: 0.67812| NDCG@10: 0.02553| HIT@10: 0.00477: 100%|██████████| 1/1 [00:07<00:00,  7.48s/it]\n",
            "Epoch:   5| Train loss: 0.67885| NDCG@10: 0.02475| HIT@10: 0.00477: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "Epoch:   6| Train loss: 0.67625| NDCG@10: 0.02163| HIT@10: 0.00447: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "Epoch:   7| Train loss: 0.67466| NDCG@10: 0.01663| HIT@10: 0.00373: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:   8| Train loss: 0.67234| NDCG@10: 0.01607| HIT@10: 0.00402: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:   9| Train loss: 0.67392| NDCG@10: 0.01364| HIT@10: 0.00343: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "Epoch:  10| Train loss: 0.67353| NDCG@10: 0.03042| HIT@10: 0.00641: 100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n",
            "Epoch:  11| Train loss: 0.66905| NDCG@10: 0.03043| HIT@10: 0.00611: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it]\n",
            "Epoch:  12| Train loss: 0.67214| NDCG@10: 0.03376| HIT@10: 0.00656: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
            "Epoch:  13| Train loss: 0.66587| NDCG@10: 0.02164| HIT@10: 0.00447: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:  14| Train loss: 0.66688| NDCG@10: 0.03453| HIT@10: 0.00686: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]\n",
            "Epoch:  15| Train loss: 0.66424| NDCG@10: 0.03954| HIT@10: 0.00954: 100%|██████████| 1/1 [00:07<00:00,  7.46s/it]\n",
            "Epoch:  16| Train loss: 0.66256| NDCG@10: 0.04186| HIT@10: 0.00864: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it]\n",
            "Epoch:  17| Train loss: 0.66196| NDCG@10: 0.03691| HIT@10: 0.00835: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n",
            "Epoch:  18| Train loss: 0.66081| NDCG@10: 0.03313| HIT@10: 0.00686: 100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n",
            "Epoch:  19| Train loss: 0.65929| NDCG@10: 0.03301| HIT@10: 0.00626: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  20| Train loss: 0.65693| NDCG@10: 0.03817| HIT@10: 0.00894: 100%|██████████| 1/1 [00:07<00:00,  7.34s/it]\n",
            "Epoch:  21| Train loss: 0.65303| NDCG@10: 0.04752| HIT@10: 0.01162: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:  22| Train loss: 0.65279| NDCG@10: 0.04951| HIT@10: 0.01043: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "Epoch:  23| Train loss: 0.64925| NDCG@10: 0.04624| HIT@10: 0.01058: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:  24| Train loss: 0.64193| NDCG@10: 0.05420| HIT@10: 0.01311: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]\n",
            "Epoch:  25| Train loss: 0.64114| NDCG@10: 0.04121| HIT@10: 0.00924: 100%|██████████| 1/1 [00:07<00:00,  7.43s/it]\n",
            "Epoch:  26| Train loss: 0.63329| NDCG@10: 0.05799| HIT@10: 0.01267: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  27| Train loss: 0.62519| NDCG@10: 0.05453| HIT@10: 0.01356: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it]\n",
            "Epoch:  28| Train loss: 0.61425| NDCG@10: 0.06370| HIT@10: 0.01401: 100%|██████████| 1/1 [00:07<00:00,  7.48s/it]\n",
            "Epoch:  29| Train loss: 0.60201| NDCG@10: 0.05568| HIT@10: 0.01297: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it]\n",
            "Epoch:  30| Train loss: 0.58885| NDCG@10: 0.04180| HIT@10: 0.00999: 100%|██████████| 1/1 [00:07<00:00,  7.34s/it]\n",
            "Epoch:  31| Train loss: 0.57546| NDCG@10: 0.04540| HIT@10: 0.00999: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "Epoch:  32| Train loss: 0.56206| NDCG@10: 0.06537| HIT@10: 0.01490: 100%|██████████| 1/1 [00:07<00:00,  7.45s/it]\n",
            "Epoch:  33| Train loss: 0.55435| NDCG@10: 0.05278| HIT@10: 0.01177: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
            "Epoch:  34| Train loss: 0.54139| NDCG@10: 0.03852| HIT@10: 0.00954: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  35| Train loss: 0.53285| NDCG@10: 0.05555| HIT@10: 0.01162: 100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
            "Epoch:  36| Train loss: 0.52235| NDCG@10: 0.05390| HIT@10: 0.01341: 100%|██████████| 1/1 [00:07<00:00,  7.41s/it]\n",
            "Epoch:  37| Train loss: 0.51531| NDCG@10: 0.05537| HIT@10: 0.01237: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "Epoch:  38| Train loss: 0.50540| NDCG@10: 0.05968| HIT@10: 0.01297: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  39| Train loss: 0.49345| NDCG@10: 0.04495| HIT@10: 0.01013: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  40| Train loss: 0.48833| NDCG@10: 0.04649| HIT@10: 0.01073: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it]\n",
            "Epoch:  41| Train loss: 0.48221| NDCG@10: 0.05001| HIT@10: 0.01177: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  42| Train loss: 0.47450| NDCG@10: 0.05429| HIT@10: 0.01148: 100%|██████████| 1/1 [00:07<00:00,  7.35s/it]\n",
            "Epoch:  43| Train loss: 0.46964| NDCG@10: 0.03942| HIT@10: 0.01013: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "Epoch:  44| Train loss: 0.46293| NDCG@10: 0.04355| HIT@10: 0.01028: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:  45| Train loss: 0.46004| NDCG@10: 0.04503| HIT@10: 0.00939: 100%|██████████| 1/1 [00:07<00:00,  7.44s/it]\n",
            "Epoch:  46| Train loss: 0.45786| NDCG@10: 0.05405| HIT@10: 0.01133: 100%|██████████| 1/1 [00:07<00:00,  7.39s/it]\n",
            "Epoch:  47| Train loss: 0.45056| NDCG@10: 0.05845| HIT@10: 0.01386: 100%|██████████| 1/1 [00:07<00:00,  7.36s/it]\n",
            "Epoch:  48| Train loss: 0.45184| NDCG@10: 0.05694| HIT@10: 0.01222: 100%|██████████| 1/1 [00:07<00:00,  7.37s/it]\n",
            "Epoch:  49| Train loss: 0.44276| NDCG@10: 0.05610| HIT@10: 0.01162: 100%|██████████| 1/1 [00:07<00:00,  7.38s/it]\n",
            "Epoch:  50| Train loss: 0.44347| NDCG@10: 0.04508| HIT@10: 0.01133: 100%|██████████| 1/1 [00:07<00:00,  7.34s/it]\n"
          ]
        }
      ],
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            make_graph_data_set = make_graph_data_set, \n",
        "            optimizer = optimizer,\n",
        "            n_batch = config.n_batch,\n",
        "            )\n",
        "        with torch.no_grad():\n",
        "            ndcg, hit = evaluate(\n",
        "                u_emb = model.u_final_embeddings.detach(), \n",
        "                i_emb = model.i_final_embeddings.detach(), \n",
        "                Rtr = R_train, \n",
        "                Rte = R_valid, \n",
        "                k = 10,\n",
        "                )\n",
        "\n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ]
    }
  ]
}