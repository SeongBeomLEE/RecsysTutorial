{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCF-for-implicit-feedback.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZvBSySgxIgP6Hlv87Qh6O06OC5ExAoHe",
      "authorship_tag": "ABX9TyN0FmlPr3rpKsDS3uVVEkWi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongBeomLEE/RecsysTutorial/blob/main/NCF/NCF_for_implicit_feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-box"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjOnv1NS52ww",
        "outputId": "f4e779a3-d501-4073-ca29-c4688011a482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-box\n",
            "  Downloading python_box-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 6.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-box\n",
            "Successfully installed python-box-6.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtBCq174_N1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import random\n",
        "from datetime import datetime\n",
        "from time import time\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from box import Box\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG8uF4zk4_N2"
      },
      "source": [
        "# 1. 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7_2AsnT4_N2"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'data_path' : \"/content/drive/MyDrive/RecsysTutorial/Data/MovieLens\" , # 데이터 경로\n",
        "\n",
        "    'model_path' : \"/content/drive/MyDrive/RecsysTutorial/model\", # 모델 저장 경로\n",
        "    'model_name' : 'GMF.pt',\n",
        "\n",
        "    'num_epochs' : 15,\n",
        "    'lr' : 0.005,\n",
        "    'batch_size' : 1024,\n",
        "\n",
        "    \"num_factor\" : 512,\n",
        "    \"num_layers\" : 3,\n",
        "    \"dropout\" : 0.2,\n",
        "\n",
        "    'valid_samples' : 10, # 검증에 사용할 sample 수\n",
        "    'seed' : 22,\n",
        "}\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Box(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc1RD4CH4_N3"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(config.model_path):\n",
        "    os.mkdir(config.model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlRF8eUo4_N3"
      },
      "source": [
        "# 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXbxctF04_N3"
      },
      "outputs": [],
      "source": [
        "class MakeCFDataSet():\n",
        "    \"\"\"\n",
        "    GraphDataSet 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.df = pd.read_csv(os.path.join(self.config.data_path, 'ratings.csv'))\n",
        "\n",
        "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
        "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
        "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
        "\n",
        "        self.df['item_idx'] = self.df['movieId'].apply(lambda x : self.item_encoder[x])\n",
        "        self.df['user_idx'] = self.df['userId'].apply(lambda x : self.user_encoder[x])\n",
        "        \n",
        "        self.exist_users = [i for i in range(self.num_user)]\n",
        "        self.exist_items = [i for i in range(self.num_item)]\n",
        "        self.user_train, self.user_valid = self.generate_sequence_data()\n",
        "\n",
        "    def generate_encoder_decoder(self, col : str) -> dict:\n",
        "        \"\"\"\n",
        "        encoder, decoder 생성\n",
        "\n",
        "        Args:\n",
        "            col (str): 생성할 columns 명\n",
        "        Returns:\n",
        "            dict: 생성된 user encoder, decoder\n",
        "        \"\"\"\n",
        "\n",
        "        encoder = {}\n",
        "        decoder = {}\n",
        "        ids = self.df[col].unique()\n",
        "\n",
        "        for idx, _id in enumerate(ids):\n",
        "            encoder[_id] = idx\n",
        "            decoder[idx] = _id\n",
        "\n",
        "        return encoder, decoder\n",
        "    \n",
        "    def generate_sequence_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        sequence_data 생성\n",
        "\n",
        "        Returns:\n",
        "            dict: train user sequence / valid user sequence\n",
        "        \"\"\"\n",
        "        users = defaultdict(list)\n",
        "        user_train = {}\n",
        "        user_valid = {}\n",
        "        for user, item, time in zip(self.df['user_idx'], self.df['item_idx'], self.df['timestamp']):\n",
        "            users[user].append(item)\n",
        "        \n",
        "        for user in users:\n",
        "            np.random.seed(self.config.seed)\n",
        "\n",
        "            user_total = users[user]\n",
        "            valid = np.random.choice(user_total, size = self.config.valid_samples, replace = False).tolist()\n",
        "            train = list(set(user_total) - set(valid))\n",
        "\n",
        "            user_train[user] = train\n",
        "            user_valid[user] = valid # valid_samples 개수 만큼 검증에 활용 (현재 Task와 가장 유사하게)\n",
        "\n",
        "        return user_train, user_valid\n",
        "\n",
        "    def neg_sampling(self, users):\n",
        "        \n",
        "        neg_sampling_cnt = 3\n",
        "        \n",
        "        def sample_neg_items_for_u(u, num):\n",
        "            neg_items = list(set(self.exist_items) - set(self.user_train[u]))\n",
        "            neg_batch = random.sample(neg_items, num)\n",
        "            return neg_batch\n",
        "        \n",
        "        _users, neg_items = [], []\n",
        "        for user in users:\n",
        "            neg_items += sample_neg_items_for_u(user, neg_sampling_cnt)\n",
        "            _users += [user] * neg_sampling_cnt\n",
        "\n",
        "        return _users, neg_items\n",
        "\n",
        "    def get_train_valid_data(self):\n",
        "        return self.user_train, self.user_valid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CFDataset(Dataset):\n",
        "    def __init__(self, user_train):\n",
        "        self.users = []\n",
        "        self.items = []\n",
        "        for user in user_train.keys():\n",
        "            self.items += user_train[user]\n",
        "            self.users += [user] * len(user_train[user])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.users)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user = self.users[idx]\n",
        "        item = self.items[idx]\n",
        "\n",
        "        return user, item"
      ],
      "metadata": {
        "id": "7W5m9zowBq5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE4_9-bJ4_N4"
      },
      "source": [
        "# 3. 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1. GMF"
      ],
      "metadata": {
        "id": "uQJURH87i5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GMF(nn.Module):\n",
        "    def __init__(self, num_user, num_item, num_factor):\n",
        "        super(GMF, self).__init__()\n",
        "        self.user_emb = nn.Embedding(num_user, num_factor)\n",
        "        self.item_emb = nn.Embedding(num_item, num_factor)\n",
        "        \n",
        "        self.predict_layer = nn.Sequential(\n",
        "            nn.Linear(num_factor, 1, bias = False)\n",
        "        )\n",
        "\n",
        "        self._init_weight_()\n",
        "    \n",
        "    def _init_weight_(self):\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
        "        for m in self.predict_layer:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "\n",
        "        output = self.predict_layer(user_emb * item_emb)\n",
        "\n",
        "        return output.view(-1)"
      ],
      "metadata": {
        "id": "6cH6SL6ahWFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-2. MLP"
      ],
      "metadata": {
        "id": "lh0xT05Vi6s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, num_user, num_item, num_factor, num_layers, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.user_emb = nn.Embedding(num_user, num_factor)\n",
        "        self.item_emb = nn.Embedding(num_item, num_factor)\n",
        "\n",
        "        MLP_modules = []\n",
        "        input_size = num_factor * 2\n",
        "        for i in range(num_layers):\n",
        "            MLP_modules.append(nn.Dropout(p = self.dropout))\n",
        "            MLP_modules.append(nn.Linear(input_size, input_size // 2))\n",
        "            MLP_modules.append(nn.ReLU())\n",
        "            input_size = input_size // 2\n",
        "        self.MLP_layers = nn.Sequential(*MLP_modules)\n",
        "\n",
        "        self.predict_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, 1, bias = False),\n",
        "        )\n",
        "\n",
        "        self._init_weight_()\n",
        "    \n",
        "    def _init_weight_(self):\n",
        "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
        "        for m in self.MLP_layers:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "        \n",
        "        for m in self.predict_layer:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        user_emb = self.user_emb(user)\n",
        "        item_emb = self.item_emb(item)\n",
        "        \n",
        "        cat_emb = torch.cat((user_emb, item_emb), -1)\n",
        "\n",
        "        output = self.MLP_layers(cat_emb)\n",
        "\n",
        "        output = self.predict_layer(output)\n",
        "\n",
        "        return output.view(-1)"
      ],
      "metadata": {
        "id": "L0GBdNzFi73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-3. NeuMF"
      ],
      "metadata": {
        "id": "PEhpOK85i8Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, GMF, MLP, num_factor):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.gmf_user_emb = GMF.user_emb\n",
        "        self.gmf_item_emb = GMF.item_emb\n",
        "\n",
        "        self.mlp_user_emb = MLP.user_emb\n",
        "        self.mlp_item_emb = MLP.item_emb\n",
        "\n",
        "        self.mlp_layer = MLP.MLP_layers\n",
        "        for i in self.mlp_layer:\n",
        "            if isinstance(i, nn.Linear):\n",
        "                out_features = i.out_features\n",
        "\n",
        "        self.predict_layer = nn.Sequential(\n",
        "            nn.Linear(num_factor + out_features, 1, bias = False),\n",
        "        )\n",
        "\n",
        "        self._init_weight_()\n",
        "    \n",
        "    def _init_weight_(self):\n",
        "        for m in self.predict_layer:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight, a=1)\n",
        "\n",
        "    def forward(self, user, item):\n",
        "        gmf_user_emb = self.gmf_user_emb(user)\n",
        "        gmf_item_emb = self.gmf_item_emb(item)\n",
        "        gmf_output = gmf_user_emb * gmf_item_emb\n",
        "\n",
        "        mlp_user_emb = self.mlp_user_emb(user)\n",
        "        mlp_item_emb = self.mlp_item_emb(item)\n",
        "        mlp_cat_emb = torch.cat((mlp_user_emb, mlp_item_emb), -1)\n",
        "        mlp_output = self.mlp_layer(mlp_cat_emb)\n",
        "        \n",
        "        cat_output = torch.cat((gmf_output, mlp_output), -1)\n",
        "\n",
        "        output = self.predict_layer(cat_output)\n",
        "\n",
        "        return output.view(-1)"
      ],
      "metadata": {
        "id": "SR5ABy5Hi9kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-4 BPR Loss"
      ],
      "metadata": {
        "id": "654nmHppDBHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BPR_Loss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BPR_Loss, self).__init__()\n",
        "    \n",
        "    def forward(self, pos, neg):\n",
        "        bpr_loss = -torch.mean(torch.log(torch.sigmoid(pos - neg)))\n",
        "        return bpr_loss"
      ],
      "metadata": {
        "id": "F3PKzgFAOxll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9vAyOS14_N5"
      },
      "source": [
        "# 4. 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8_nloUG4_N5"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, criterion, optimizer, make_cf_data_set):\n",
        "    model.train()\n",
        "    loss_val = 0\n",
        "\n",
        "    for users, items in data_loader:\n",
        "        neg_users, neg_items = make_cf_data_set.neg_sampling(users.numpy().tolist())\n",
        "\n",
        "        all_users = torch.concat([users, torch.tensor(neg_users)]).to(device)\n",
        "        all_items = torch.concat([items, torch.tensor(neg_items)]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(all_users, all_items)\n",
        "        pos_output, neg_output = torch.split(output, [len(users), len(neg_users)])\n",
        "        pos_output = torch.concat([pos_output.view(-1, 1), pos_output.view(-1, 1), pos_output.view(-1, 1)], dim = 1).view(-1)\n",
        "        loss = criterion(pos_output, neg_output)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val += loss.item()\n",
        "\n",
        "    loss_val /= len(data_loader)\n",
        "\n",
        "    return loss_val\n",
        "\n",
        "def get_ndcg(pred_list, true_list):\n",
        "    ndcg = 0\n",
        "    for rank, pred in enumerate(pred_list):\n",
        "        if pred in true_list:\n",
        "            ndcg += 1 / np.log2(rank + 2)\n",
        "    return ndcg\n",
        "\n",
        "# 대회 메트릭인 recall과 동일\n",
        "def get_hit(pred_list, true_list):\n",
        "    hit_list = set(true_list) & set(pred_list)\n",
        "    hit = len(hit_list) / len(true_list)\n",
        "    return hit\n",
        "\n",
        "def evaluate(model, user_train, user_valid, make_cf_data_set):\n",
        "    model.eval()\n",
        "\n",
        "    NDCG = 0.0 # NDCG@10\n",
        "    HIT = 0.0 # HIT@10\n",
        "\n",
        "    all_users = make_cf_data_set.exist_users\n",
        "    all_items = make_cf_data_set.exist_items\n",
        "    with torch.no_grad():\n",
        "        for user in all_users:\n",
        "            users = [user] * len(all_items)\n",
        "            users, items = torch.tensor(users).to(device), torch.tensor(all_items).to(device)\n",
        "\n",
        "            output = model(users, items)\n",
        "            output = output.softmax(dim = 0)\n",
        "            output[user_train[user]] = -1.\n",
        "\n",
        "            uv = user_valid[user]\n",
        "            up = output.argsort()[-10:].cpu().numpy().tolist()\n",
        "\n",
        "            NDCG += get_ndcg(pred_list = up, true_list = uv)\n",
        "            HIT += get_hit(pred_list = up, true_list = uv)\n",
        "\n",
        "    NDCG /= len(all_users)\n",
        "    HIT /= len(all_users)\n",
        "\n",
        "    return NDCG, HIT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 학습"
      ],
      "metadata": {
        "id": "1IyZM-LhIBKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_cf_data_set = MakeCFDataSet(config = config)\n",
        "user_train, user_valid = make_cf_data_set.get_train_valid_data()"
      ],
      "metadata": {
        "id": "F6DySddGDeXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_dataset = CFDataset(user_train = user_train)\n",
        "data_loader = DataLoader(\n",
        "    cf_dataset, \n",
        "    batch_size = config.batch_size, \n",
        "    shuffle = True, \n",
        "    drop_last = False)"
      ],
      "metadata": {
        "id": "o8uyM8cWDrfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-1. GMF"
      ],
      "metadata": {
        "id": "5CbFZTXAIDmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GMF(\n",
        "    num_user = make_cf_data_set.num_user, \n",
        "    num_item = make_cf_data_set.num_item, \n",
        "    num_factor = config.num_factor).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
        "criterion = BPR_Loss()"
      ],
      "metadata": {
        "id": "oY3Z5ufRIWgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            data_loader = data_loader, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            make_cf_data_set = make_cf_data_set\n",
        "            )\n",
        "        \n",
        "        ndcg, hit = evaluate(model, user_train, user_valid, make_cf_data_set)\n",
        "        \n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, config.model_name))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "-ZIR4biIIGaf",
        "outputId": "cb53877a-290f-4c71-a27b-6f1dd2f8f436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.48790| NDCG@10: 0.27005| HIT@10: 0.06647: 100%|██████████| 1/1 [01:01<00:00, 61.58s/it]\n",
            "Epoch:   2| Train loss: 0.19521| NDCG@10: 0.40536| HIT@10: 0.09866: 100%|██████████| 1/1 [00:58<00:00, 58.91s/it]\n",
            "Epoch:   3| Train loss: 0.08507| NDCG@10: 0.43686| HIT@10: 0.10224: 100%|██████████| 1/1 [00:51<00:00, 51.90s/it]\n",
            "Epoch:   4| Train loss: 0.04422| NDCG@10: 0.44783| HIT@10: 0.10671: 100%|██████████| 1/1 [00:51<00:00, 51.17s/it]\n",
            "Epoch:   5| Train loss: 0.02993| NDCG@10: 0.44954| HIT@10: 0.10715: 100%|██████████| 1/1 [00:50<00:00, 50.22s/it]\n",
            "Epoch:   6| Train loss: 0.02163| NDCG@10: 0.43496| HIT@10: 0.10522: 100%|██████████| 1/1 [00:51<00:00, 51.97s/it]\n",
            "Epoch:   7| Train loss: 0.01670| NDCG@10: 0.47004| HIT@10: 0.10999: 100%|██████████| 1/1 [00:50<00:00, 50.55s/it]\n",
            "Epoch:   8| Train loss: 0.01382| NDCG@10: 0.44355| HIT@10: 0.10805: 100%|██████████| 1/1 [00:50<00:00, 50.31s/it]\n",
            "Epoch:   9| Train loss: 0.01136| NDCG@10: 0.46517| HIT@10: 0.10835: 100%|██████████| 1/1 [00:50<00:00, 50.45s/it]\n",
            "Epoch:  10| Train loss: 0.01005| NDCG@10: 0.43773| HIT@10: 0.10775: 100%|██████████| 1/1 [00:50<00:00, 50.66s/it]\n",
            "Epoch:  11| Train loss: 0.00827| NDCG@10: 0.45669| HIT@10: 0.10849: 100%|██████████| 1/1 [00:50<00:00, 50.44s/it]\n",
            "Epoch:  12| Train loss: 0.00740| NDCG@10: 0.46999| HIT@10: 0.11043: 100%|██████████| 1/1 [00:54<00:00, 54.03s/it]\n",
            "Epoch:  13| Train loss: 0.00688| NDCG@10: 0.43053| HIT@10: 0.10626: 100%|██████████| 1/1 [00:54<00:00, 54.75s/it]\n",
            "Epoch:  14| Train loss: 0.00614| NDCG@10: 0.43212| HIT@10: 0.10417: 100%|██████████| 1/1 [00:54<00:00, 54.23s/it]\n",
            "Epoch:  15| Train loss: 0.00589| NDCG@10: 0.43456| HIT@10: 0.10343: 100%|██████████| 1/1 [00:58<00:00, 58.75s/it]\n",
            "  0%|          | 0/1 [00:37<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6b4935355c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmake_cf_data_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cf_data_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-68ad77d63825>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, make_cf_data_set)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mneg_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cf_data_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mall_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0ce5a8d26bc2>\u001b[0m in \u001b[0;36mneg_sampling\u001b[0;34m(self, users)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0m_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mneg_items\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_neg_items_for_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0m_users\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0ce5a8d26bc2>\u001b[0m in \u001b[0;36msample_neg_items_for_u\u001b[0;34m(u, num)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msample_neg_items_for_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mneg_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist_items\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mneg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-2. MLP"
      ],
      "metadata": {
        "id": "ufTeiQr5IGeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(\n",
        "    num_user = make_cf_data_set.num_user, \n",
        "    num_item = make_cf_data_set.num_item,\n",
        "    num_factor = config.num_factor,\n",
        "    num_layers = config.num_layers,\n",
        "    dropout = config.dropout,).to(device)\n",
        "    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
        "criterion = BPR_Loss()"
      ],
      "metadata": {
        "id": "9Z4YUSZ6JU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            data_loader = data_loader, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            make_cf_data_set = make_cf_data_set\n",
        "            )\n",
        "        \n",
        "        ndcg, hit = evaluate(model, user_train, user_valid, make_cf_data_set)\n",
        "        \n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, \"MLP.pt\"))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f53640-52d0-4924-f393-6eeb9d729e5a",
        "id": "tI_XbvCTJU9I"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.36789| NDCG@10: 0.24865| HIT@10: 0.06036: 100%|██████████| 1/1 [01:01<00:00, 61.69s/it]\n",
            "Epoch:   2| Train loss: 0.30033| NDCG@10: 0.24330| HIT@10: 0.05618: 100%|██████████| 1/1 [01:00<00:00, 60.61s/it]\n",
            "Epoch:   3| Train loss: 0.26206| NDCG@10: 0.25007| HIT@10: 0.05961: 100%|██████████| 1/1 [01:00<00:00, 60.55s/it]\n",
            "Epoch:   4| Train loss: 0.21405| NDCG@10: 0.31328| HIT@10: 0.07303: 100%|██████████| 1/1 [01:00<00:00, 60.25s/it]\n",
            "Epoch:   5| Train loss: 0.18037| NDCG@10: 0.34407| HIT@10: 0.07973: 100%|██████████| 1/1 [01:00<00:00, 60.53s/it]\n",
            "Epoch:   6| Train loss: 0.15760| NDCG@10: 0.38236| HIT@10: 0.09031: 100%|██████████| 1/1 [01:01<00:00, 61.77s/it]\n",
            "Epoch:   7| Train loss: 0.14214| NDCG@10: 0.40384| HIT@10: 0.09463: 100%|██████████| 1/1 [00:59<00:00, 59.72s/it]\n",
            "Epoch:   8| Train loss: 0.13183| NDCG@10: 0.37171| HIT@10: 0.09046: 100%|██████████| 1/1 [00:59<00:00, 59.38s/it]\n",
            "Epoch:   9| Train loss: 0.12116| NDCG@10: 0.36791| HIT@10: 0.08852: 100%|██████████| 1/1 [00:59<00:00, 59.66s/it]\n",
            "Epoch:  10| Train loss: 0.11330| NDCG@10: 0.39471| HIT@10: 0.09344: 100%|██████████| 1/1 [00:59<00:00, 59.98s/it]\n",
            "Epoch:  11| Train loss: 0.10672| NDCG@10: 0.40129| HIT@10: 0.09583: 100%|██████████| 1/1 [00:59<00:00, 59.85s/it]\n",
            "Epoch:  12| Train loss: 0.10034| NDCG@10: 0.38354| HIT@10: 0.09270: 100%|██████████| 1/1 [00:59<00:00, 59.81s/it]\n",
            "Epoch:  13| Train loss: 0.09541| NDCG@10: 0.40960| HIT@10: 0.09791: 100%|██████████| 1/1 [01:01<00:00, 61.03s/it]\n",
            "Epoch:  14| Train loss: 0.09182| NDCG@10: 0.41145| HIT@10: 0.09687: 100%|██████████| 1/1 [00:56<00:00, 56.75s/it]\n",
            "Epoch:  15| Train loss: 0.08844| NDCG@10: 0.37085| HIT@10: 0.09076: 100%|██████████| 1/1 [00:56<00:00, 56.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-3. NMF"
      ],
      "metadata": {
        "id": "gOweiB01IK9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmf = GMF(\n",
        "    num_user = make_cf_data_set.num_user, \n",
        "    num_item = make_cf_data_set.num_item, \n",
        "    num_factor = config.num_factor).to(device)\n",
        "\n",
        "gmf.load_state_dict(torch.load(os.path.join(config.model_path, f'GMF.pt')))\n",
        "\n",
        "mlp = MLP(\n",
        "    num_user = make_cf_data_set.num_user, \n",
        "    num_item = make_cf_data_set.num_item,\n",
        "    num_factor = config.num_factor,\n",
        "    num_layers = config.num_layers,\n",
        "    dropout = config.dropout,).to(device)\n",
        "\n",
        "mlp.load_state_dict(torch.load(os.path.join(config.model_path, f'MLP.pt')))\n",
        "\n",
        "model = NeuMF(\n",
        "    GMF = gmf, \n",
        "    MLP = mlp, \n",
        "    num_factor = config.num_factor).to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = config.lr, momentum = 0.9)\n",
        "criterion = BPR_Loss()"
      ],
      "metadata": {
        "id": "wyIKccKeC_wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hit = 0\n",
        "for epoch in range(1, config.num_epochs + 1):\n",
        "    tbar = tqdm(range(1))\n",
        "    for _ in tbar:\n",
        "        train_loss = train(\n",
        "            model = model, \n",
        "            data_loader = data_loader, \n",
        "            criterion = criterion, \n",
        "            optimizer = optimizer, \n",
        "            make_cf_data_set = make_cf_data_set\n",
        "            )\n",
        "        \n",
        "        ndcg, hit = evaluate(model, user_train, user_valid, make_cf_data_set)\n",
        "        \n",
        "        if best_hit < hit:\n",
        "            best_hit = hit\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, \"NMF.pt\"))\n",
        "\n",
        "        tbar.set_description(f'Epoch: {epoch:3d}| Train loss: {train_loss:.5f}| NDCG@10: {ndcg:.5f}| HIT@10: {hit:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySeJn8IBK3bP",
        "outputId": "f97a10c6-134c-4a7b-9fe1-509a7ff39eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   1| Train loss: 0.15745| NDCG@10: 0.36547| HIT@10: 0.08495: 100%|██████████| 1/1 [01:02<00:00, 62.08s/it]\n",
            "Epoch:   2| Train loss: 0.09142| NDCG@10: 0.39837| HIT@10: 0.09568: 100%|██████████| 1/1 [01:01<00:00, 61.00s/it]\n",
            "Epoch:   3| Train loss: 0.08521| NDCG@10: 0.41296| HIT@10: 0.09806: 100%|██████████| 1/1 [01:03<00:00, 63.78s/it]\n",
            "Epoch:   4| Train loss: 0.08137| NDCG@10: 0.40306| HIT@10: 0.09732: 100%|██████████| 1/1 [01:02<00:00, 62.35s/it]\n",
            "Epoch:   5| Train loss: 0.08098| NDCG@10: 0.41669| HIT@10: 0.09896: 100%|██████████| 1/1 [01:00<00:00, 60.96s/it]\n",
            "Epoch:   6| Train loss: 0.07980| NDCG@10: 0.42041| HIT@10: 0.10060: 100%|██████████| 1/1 [01:01<00:00, 61.50s/it]\n",
            "Epoch:   7| Train loss: 0.07746| NDCG@10: 0.42372| HIT@10: 0.10104: 100%|██████████| 1/1 [01:02<00:00, 62.46s/it]\n",
            "Epoch:   8| Train loss: 0.07755| NDCG@10: 0.41844| HIT@10: 0.10134: 100%|██████████| 1/1 [01:10<00:00, 70.55s/it]\n",
            "Epoch:   9| Train loss: 0.07628| NDCG@10: 0.42664| HIT@10: 0.10194: 100%|██████████| 1/1 [01:01<00:00, 61.49s/it]\n",
            "Epoch:  10| Train loss: 0.07555| NDCG@10: 0.42315| HIT@10: 0.10209: 100%|██████████| 1/1 [01:02<00:00, 62.29s/it]\n",
            "Epoch:  11| Train loss: 0.07351| NDCG@10: 0.43463| HIT@10: 0.10358: 100%|██████████| 1/1 [01:02<00:00, 62.06s/it]\n",
            "Epoch:  12| Train loss: 0.07310| NDCG@10: 0.43857| HIT@10: 0.10432: 100%|██████████| 1/1 [01:02<00:00, 62.55s/it]\n",
            "Epoch:  13| Train loss: 0.07146| NDCG@10: 0.44066| HIT@10: 0.10477: 100%|██████████| 1/1 [01:00<00:00, 60.66s/it]\n",
            "Epoch:  14| Train loss: 0.06988| NDCG@10: 0.43293| HIT@10: 0.10417: 100%|██████████| 1/1 [01:00<00:00, 60.61s/it]\n",
            "Epoch:  15| Train loss: 0.06953| NDCG@10: 0.43976| HIT@10: 0.10522: 100%|██████████| 1/1 [01:01<00:00, 61.39s/it]\n"
          ]
        }
      ]
    }
  ]
}